\saveSpace\saveSpace\section{Formalization}\label{sec:formal}
\saveSpace\saveSpace

Here we show a simple formalization for the language we presented so far.
We also model nested classes, but in order to avoid uninteresting complexities, we assume that
all the type names are fully qualified from the top level, so the examples shown before should be
written using \Q@This.Exp@, \Q@This.Sum@ and so on.
In a real language, a simple pre-processor may take care of this step.

In most languages, when implementing an interface, the programmer may avoid repeating abstract methods
they do not wish to implement.
In the same spirit of before, in this simplified formalization we consider source code containing
all the imported abstract methods. In a real language, a normalization process
may hide this abstraction.

We also consider a binary operator sum \Q@+@ instead of the variable arity operator \Q@use@.

Figure 1 contains the complete formalization for the 
compilation process and the type system for \name.
It starts with the syntax, then
we show the compilation process and the typing rules.




\begin{figure}
%NEW FORMALISATION below
% Syntax
% D::=TD|CD
% TE::=t:E Trait Decl Expr
% CE::=C:E Class Decl
% TD::=t:L
% CD::=C:L
% E::= L| t| E+E | E[rename T.m1->m2]|E[rename T1->T2]|E[redirect T1->T2]
% L::= {interface? implements Ts Ms}//all L are like LC in 42
% T::=C|C.T // .T is a shortcut for This.T
% M::= static? method T m(T1 x1..Tn xn) e? | CD
% e::= x| e.m(es) | T.m(es)

\begin{bnf}
\prodFull\mID{\mt\mid\mC}{class or trait name}\\
\prodFull\mDE{\mID\terminalCode{=}\mE}{Meta-declaration}\\
\prodFull\mD{\mID\terminalCode{=}\mL}{Declaration}\\
\prodFull\mE{\mL \mid \mt \mid \mE\,\terminalCode{+}\mE
\mid \ldots
%\mid \mE\terminalCode{[rename}\ \mT\terminalCode{.}\mm_1\ \terminalCode{to}\ \mm_2\terminalCode{]}
}{Code Expression}\\
%\prodNextLine{
% \mid
%\mE\terminalCode{[rename}\ \mT_1\ \terminalCode{in}\ \mT_2\terminalCode{]} \mid
%\mE\terminalCode{[redirect}\ \mT_1\ \terminalCode{to}\ \mT_2\terminalCode{]}}{Code Expression}\\
\prodFull\mL{
\oC \Opt{\terminalCode{interface}}\ \terminalCode{implements} \overline\mT\ \overline\mM\ \cC}{Code Literal}\\
\prodFull\mT{\mC \mid \mC\terminalCode{.}\mT}{Type}\\
\prodFull\mM{\Opt{\terminalCode{static}}\ \terminalCode{method}\ \mT\ \mm\oR\overline{\mT\,\mx}\cR \Opt\me \mid \mC\terminalCode{=}\mL}{Member}\\

\prodFull\me{\mx \mid \me\terminalCode{.}\mm\oR\overline\me\cR \mid \mT\terminalCode{.}\mm\oR\overline\me\cR}{Expression}\\

\prodFull{v_{{\smallDs}}}{\mT\terminalCode{.}\mm\oR\overline{v}\cR
\text{  where }\mm \text{ is abstract in }\overline\mD(\mT)
}{value}\\
\prodFull{\ctx_{\smallDs}}{[]\mid
\ctx\terminalCode{.}\mm\oR\overline\me\cR
\mid
v_{{\smallDs}}\terminalCode{.}\mm\oR\overline{v_{{\smallDs}}},\ctx,\overline\me\cR
\mid
\mT\terminalCode{.}\mm\oR\overline{v_{{\smallDs}}},\ctx,\overline\me\cR
}{evaluation ctx}\\
\prodFull{\ctx_c}{[]\mid\ctx\,\terminalCode+\mE | \mL\,\terminalCode+\ctx |\ldots}{compilation ctx}\\
\prodFull{\ctx}{[]\mid\ctx\,\terminalCode+\mE | \mE\,\terminalCode+\ctx |\ldots}{ctx}\\
\end{bnf}\\
\\
\newcommand{\pushLeft}{\!\!\!\!\!\!\!\!\!\!\!\!}
$\begin{array}{l}

%       D.E -->^+_CDs L  CDs|-CD1:OK .. CDs|-CDn:OK       CDs=CD1..CDn
% (top)---------------------------------------------------------------    D.E not of form L
%      CD1..CDn CDs' D Ds -> CDs CDs' D[with E=L] Ds

\pushLeft\inferrule[(top)]{
  \mE_0 \xrightarrow[\smallDs]{} \mE_1
  \\
  \forall \mD\in\overline\mD,
  \overline\mD\vdash\mD:\text{OK}
  }{ 
    \overline\mD \ \overline{\mD'}\ \mID\terminalCode{=}\mE_0 \ \overline{\mDE}
    \rightarrow 
    \overline\mD\ \overline{\mD'}\ \mID\terminalCode{=}\mE_1\ \overline{\mDE}
  } %{\overline\mD=\mD_1..\mD_n }
\quad\quad
%
%     ------------------------
%      t -->_CDs CDs(t)

 \inferrule[(look-up)]{
    \ 
  }{ 
    \mt \xrightarrow[\smallDs]{}\ \overline\mD(\mt)
  }
\quad

\inferrule[(ctx-c)]{
    \mE_0 \xrightarrow[\smallDs]{}\ \mE_1
  }{ 
     {\ctx}_c[\mE_0] \xrightarrow[\smallDs]{}\ {\ctx}_c[\mE_1]
  }
\quad
%
%      --------------------------      L = L1+L2
%      L1+L2  -->_CDs L

\inferrule[(sum)]{
    \
  }{ 
     \mL_1\,\terminalCode{+}\mL_2 \xrightarrow[\smallDs]{}\ \mL
  }\mL = \mL_1+\mL_2
\\[5ex]
%  C;CDs,C=L |- L[This=C] :OK
% ----------------------------------------- coherent(L)
%  CDs|-C=L : OK

\pushLeft\inferrule[(CD-OK)]{
    \mC;\overline\mD,\mC\terminalCode{=}\mL\vdash \mL[\terminalCode{This}=\mC]\ :\text{OK}
  }{ 
     \overline\mD \vdash \mC\terminalCode{=}\mL\ :\text{OK}
  } \text{coherent}(\mC,\mL)
\quad\quad 

%    This;CDs,This=L |- L :OK
%----------------------------------------
%    CDs|-t=L : OK

\inferrule[(TD-OK)]{
    \terminalCode{This};\overline\mD,\terminalCode{This=}\mL\vdash \mL\ :\text{OK}
  }{ 
     \overline\mD \vdash \mt\terminalCode{=}\mL\ :\text{OK}
  }
\\[5ex] 

%  forall i in 1..k T;CDs|-Mi:Ok
%--------------------------------------------------  L={interface? implements T1..Tn M1..Mk} 
%  T;CDs|-L:Ok                                         forall i in 1..n 	CDs(Ti).interface?=interface
%                                                             forall i in 1..n and m in 	dom(CDs(Ti)), m in dom(L)

\pushLeft\inferrule[(L-OK)]{
    \forall\mM\in\overline\mM,
  \mT;\overline\mD\vdash\mM:\text{OK}
  }{ 
     \mT;\overline\mD \vdash \mL\ :\text{OK}
  } \begin{array}{l} 
  \mL=\oC \Opt{\terminalCode{interface}}\ \terminalCode{implements} \overline\mT \ \overline\mM \cC \\
  \forall \mT\in\overline\mT \text{and } m \in \dom(\mD(\mT)), \mm \in \dom(\mL)
   \end{array}
\quad
\inferrule[(Nested-OK)]{
    \mT\terminalCode{.}\mC;\overline\mD\vdash \mL\ :\text{OK}
  }{ 
     \mT;\overline\mD \vdash \mC\terminalCode{=}\mL\ :\text{OK}
  }

\\[5ex] 

%  if e?=e then CDs; G|-e:T                         
%----------------------------------------------------------   forall T in CDs(C).Ts, if m in dom(CDs(Ti)) then
%   T;CDs|-static? T0 m(T1 x1..Tn xn) e?              static? T0 m(T1 x1..Tn xn) in CDs(Ti)
%                                                                        if static?=static then G=x1:T1 .. xn:Tn
%                                                                        else G=this:T,x1:T1 .. xn:Tn

\pushLeft\inferrule[(Method-OK)]{
    \text{if}\ \Opt\me=\me\ \text{then}\ \overline\mD; \mG\vdash\me:\mT
  }{ 
     \mT;\overline\mD \vdash \Opt{\terminalCode{static}}\ \terminalCode{method}\ \mT_0\ \mm\oR\mT_1\,\mx_1\ldots\mT_n\,\mx_n\cR \Opt\me
  } \begin{array}{l} 
  \text{if}\ \Opt{\terminalCode{static}}=\terminalCode{static}\\
  \quad \text{then}\ \mG=\mx_1:\mT_1\ .. \ \mx_n:\mT_n\ \\
  \quad\text{else}\ \mG=\terminalCode{this}:\mT,\mx_1:\mT_1\ ..\ \mx_n:\mT_n
  \\
%removed, now is well formedness
%  \forall \mT \in \text{implementsOf}(\overline\mD(\mC)),\ \text{if}\ \mm \in \dom(\overline\mD(\mT))\ \text{then} \\
%  \quad\Opt{\terminalCode{static}}\ \terminalCode{method}\ \mT_0\ \mm\oR\overline{\mT\,\mx}\cR \in \overline\mD(\mT) \\
   \end{array}
\\[5ex] 



\pushLeft\inferrule[(Subsumption)]{
%  \begin{array}{l}
    \overline\mD; \mG\vdash\me: \mT_1  \\\\
    \overline\mD\vdash\mT_1 \leq \mT_2
%  \end{array}
  }{ 
     \overline\mD; \mG\vdash\me: \mT_2
  }
\quad \inferrule[(Method-Call)]{
    \mD;\mG\vdash\me_1:\mT_1\ .. \ \mD;\mG\vdash\me_n:\mT_n
  }{ 
    \mD;\mG\vdash \mT_0.\mm\oR\me_1\ .. \ \me_n\cR:\mT
  } \terminalCode{method}\ \mT\ \mm\oR\overline{\mT\,\mx}\cR \text{\_} \in \overline\mD\oR\mT_0 \cR
\\[5ex] 

%    CDs;G|-e0:T0 .. CDs;G|-en:Tn
%---------------------------------------------    static T m(T1 x1..Tn xn) _ in CDs(T0)
%  CDs;G|-e0.m(e1..en):T

\pushLeft\inferrule[(X)]{
    \
  }{ 
    \overline\mD; \mG\vdash\mx: \mG\oR\mx\cR
  }
\quad
\inferrule[(Static-Method-Call)]{
    \mD;\mG\vdash\me_0:\mT_0\ .. \ \mD;\mG\vdash\me_n:\mT_n
  }{ 
    \mD;\mG\vdash \me_0.\mm\oR\me_1\ .. \ \me_n\cR:\mT
  } \terminalCode{static method}\ \mT\ \mm\oR\overline{\mT\,\mx}\cR \text{\_} \in \overline\mD\oR\mT_0 \cR

\\[5ex] 
\pushLeft\inferrule[(ctxv)]{\me_0\xrightarrow[\smallDs]{}\me_1}{
 \ctx_{\smallDs}[\me_0]\xrightarrow[\smallDs]{} \ctx_{\smallDs}[\me_1]
 }

\quad
\inferrule[(s-m)]{{}_{}}{
 \mT\terminalCode{.}\mm\oR\overline\vds\cR\xrightarrow[\smallDs]{}
 \text{meth}(\overline\mD(\mT,\mm),\overline\vds)
}
\quad
\inferrule[(m)]{{}_{}}{
 \vds\terminalCode{.}\mm\oR\overline\vds\cR\xrightarrow[\smallDs]{}
 \text{meth}(\overline\mD(\mT,\mm),\vds\,\overline\vds)
}\vds=\mT\terminalCode{.}\mm'\oR\_\cR\\
\end{array}
$\\
\caption{Formalization}
\end{figure}

\subsection{Syntax}

In the following section, we present a simplified grammar of \name:
\noindent We use $\mt$ and $\mC$ to represent lower case trait and upper case class identifiers respectively.

\noindent To declare a trait \mTD\ or a class \mCD, we can use either a code literal \mL\ or a trait
expression $\mE$.
Note how in $\mE$\ you can refer to a trait by name.

 In full 42, we support various operators including the ones presented before,
 but here we only show the single operator \Q@+@.
%This operation is a generalization to the case of nested classes of the simplest and most elegant
%trait composition operator~\cite{ducasse2006traits}.


Code literals \mL\ can be marked as interfaces. We use '?' to represent optional terms.
Note that the interface keyword is inside the curly brackets,
so an upper case name associated with an interface literal is a class-interface, while a lowercase one is a trait-interface.

Then we have a set of implemented interfaces and a set of member
declarations, they can be methods or nested classes.
The members of a code literal are a set, thus their order is immaterial.

\noindent If a code literal implements zero interfaces, the concrete syntax omits the \Q@implements@ keyword.

Methods \mMD~can be instance methods or \Q@static@ methods. 
A static method in \name is similar to a \Q@static@ method in Java but can be abstract.
This is very useful in the context of code composition.
To denote a method as abstract, instead of an optional keyword we just omit the implementation \me.

Finally, expression $\me$ are just variables, method calls or static method calls.
The ugliness of having two different kinds of method call is an artefact of our simplifications.
In the full language, type names are a kind of expression whose type helps to model metaclasses.

Our concept of abstract state implies we have no \Q@new@ expressions, and
the values are just calls to abstract static methods.
Thus values are parametric on the shape of the specific programs $\overline\mD$.

We then show the evaluation context, the compilation context and full context.

\subsection{Well-formedness}

The whole program, that is $\overline\mDE$, is well formed if
all the traits and classes at top level have unique names, and the special class name
\Q@This@ is reserved and if all
the subtype relations are consistent:
this means that the implementation of interfaces is not circular,
and that $\forall\ \_\terminalCode{=}\ctx[\mL]\in\overline\mDE, \text{consistentSubtype}(\overline\mDE,\terminalCode{This=}\mL,\mL)$

\noindent That is, every literal declares (abstract or not) all the methods that are declared 
in its super interfaces.


\noindent\textbf{Define }$\text{consistentSubtype}(\overline\mDE,\mL)$\\
$\begin{array}{l}
\!\!\!\bullet\ \text{consistentSubtype}(
  \overline\mDE,
  \oC
  \Opt{\terminalCode{interface}}
  \terminalCode{implements}\overline\mT\ 
  \overline\mM
  \cC
  )\quad\text{where}\\
\quad\quad \forall\ \_\terminalCode{=}\mL\in  \overline\mM, 
\text{consistentSubtype}(\overline\mDE,\mL) \text{ and }
\\
\quad\quad 
\forall \terminalCode{method}\ \mT_0 \mm\oR
\mT_1\,\mx_1\ldots\mT_n\,\mx_n
%\overline{\mT\,\mx}
\cR\Opt\me
\in\overline\mM,
\forall \mT\in\overline\mT,
\text{if }\overline\mDE(\mT,\mm)=\mT'_0 \mm\oR
%\overline{\mT\,\mx}'
\mT'_1\,\mx'_1\ldots\mT'_k\,\mx'_k
\cR
\\\quad\quad\text{ then }
%\mT_0=\mT'_0, \overline{\mT\,\mx}=\overline{\mT\,\mx}'
\mT_0\ldots\mT_n=\mT'_0\ldots\mT'_k

\text{ and }\overline\mDE(\mT)=\oC\terminalCode{interface}\,\_\cC

\\
\end{array}$

\noindent An \mL\ is well formed if:
\begin{itemize}
\item all method parameters have unique names and the special parameter name \Q@this@ is not declared
 in the parameter list,
\item all methods in a code literal have unique names,
\item all nested classes have unique names, and no nested class is called \Q@This@,
\item all used variables are in scope, and
\item all methods in an interface are abstract, and there are no interface static methods.
\end{itemize}


\noindent While classes are typed assuming \lstinline{this} is of the nominal type of the
class, trait declarations, do not introduce any nominal types.  \lstinline{this}
in a trait is typed with a special type \lstinline{This} that is visible only
inside such trait. Syntactically, \Q@This@ is just a special, reserved, class name $\mC$.
A Literal can use the \lstinline{This} type,
and when flattening completes creating a class definition, \Q@This@ will be replaced with such class name.


\subsection{Compilation process}

Usually the compilation process is not modelled, but here it is the \textbf{most interesting part}.
Here we explain in the detail the flattening process and how and when compilation errors may arise.
It is composed by rules \Rulename{top},\ \Rulename{look-up},\ \Rulename{ctx-c} and \Rulename{sum}.
If we were to formally model more composition operators, each operator would have its own  rule.

Rule \Rulename{top}
compiles the leftmost top level (trait or class) declaration that needs to be compiled.
In order to do so,
it identifies the subset of the program $\overline\mD$ that can already be typed (second premise).
Then the expression is executed under the control of such compiled program (first premise).
According to rule \Rulename{look-up}, all the traits inside the expression needs to
be compiled, that is $\forall\mt. \mE=\ctx[\mt], \mt\in\dom(\overline\mD)$.
If a large enough $\overline\mD$ cannot be typed, this would cause a compilation error
at this stage.

Rule \Rulename{look-up}
replaces a trait name $\mt$ with the corresponding literal $\mL$.
Thanks to the fact that $\overline\mD$ is all well typed, we know that $\mL$ is well typed too.

Rule \Rulename{ctx-c}
uses the compilation context to decide what step to apply next.
It enforces a deterministic left-right call by value\footnote{
In the flattening process, values are code literals $\mL$.} reduction;
thus the leftmost invalid sum that is performed will be the one providing the compilation error.

Finally, rule \Rulename{sum}
applies the operator, as follows:

\noindent\textbf{Define }$\mL_1+\mL_2, \ \overline{\mM}+\overline{\mM},\ \mM+\mM$\\
$\begin{array}{l}
\!\!\!\bullet\ \mL_1+\mL_2 =\mL_3\quad\text{where}\\
\quad\quad \mL_1= \oC \Opt{\terminalCode{interface}}\ \terminalCode{implements} \overline\mT_1\ \overline\mM_1\ \overline\mM_0\cC\\
\quad\quad \mL_2= \oC \Opt{\terminalCode{interface}}\ \terminalCode{implements} \overline\mT_2\ \overline\mM_2\ \overline\mM_0\cC\\
\quad\quad \mL_3= \oC \Opt{\terminalCode{interface}}\ \terminalCode{implements} \overline\mT_1,\overline\mT_2\ \overline\mM_1,\overline\mM_2\ (\overline\mM_0+\overline\mM_0')\cC\\
\quad\quad \dom(\overline\mM_1)\pitchfork \dom(\overline\mM_2) \text{ and } \dom(\overline\mM_0)\ =\ \dom(\overline\mM_0')\\

\!\!\!\bullet\ \mM_1..\mM_n+\mM'_1+\mM'_n\ = \ \mM_1+\mM'_1..\mM_n+\mM'_n\\

\!\!\!\bullet\ \mC\terminalCode{=}\mL_1+\mC\terminalCode{=}\mL_2\ = \ \mC\terminalCode{=}\mL_3\quad if \mL_1+\mL_2\\

\!\!\!\bullet\ \mM_1+\mM_2=\mM_2+\mM_1\\

\!\!\!\bullet\ \Opt{\terminalCode{static}}\ \terminalCode{method}\ \mT_0\ \mm\oR\overline{\mT\,\mx}\cR \ + \ \Opt{\terminalCode{static}}\ \terminalCode{method}\ \mT_0\ \mm\oR\overline{\mT\,\mx}\cR \Opt\me = \Opt{\terminalCode{static}}\ \terminalCode{method}\ \mT_0\ \mm\oR\overline{\mT\,\mx}\cR \Opt\me\\
\end{array}$

It composes the content of the arguments
by taking the union of their members and the union of their \Q@implements@.
Members with the same name are recursively composed.
There are three cases where the composition is impossible.
\begin{itemize}
\item MethodClash: two methods with the same name are composed,
but either their headers have different types or they are both implemented.
\item ClassClash: a class is composed with an interface.%
\footnote{
The full language offers some relaxation here, so that for example an empty class can be seen as an empty interface during composition.
}
\item ImplementsClash:
The result code would not respect implements relationships.
For example
\begin{lstlisting}
t1={
  A:{interface method Void a()}
  B:{}
   }
t2={
  A:{interface}
  B:{implements A}
  }
\end{lstlisting}
Naively, \Q@t1+t2@ should result in a class \Q@B@ implementing \Q@A@ with method \Q@a()@,
but \Q@B@ would not offer such method \Q@a()@.
While in \name it could be possible to try to patch class \Q@B@, for example adding a
abstract method \Q@a()@, we chose to give an error in this case, since in the full 42 language
such patch would 
be able to turn private nested classes
into abstract (private) ones.

ImplementsClash can happen only when composing nested interfaces. Note that while the first two kind of errors are obtained directly by the definition of 
$\mL_1+\mL_2$, ImplementsClash is obtained since injecting the resulting 
$\mL$ in the program would make it ill-formed by 
$\text{consistentSubtype}(\overline\mDE,\mL)$.
\end{itemize}

\subsection{Typing}
Typing is composed by rules \Rulename{cd-ok}, \Rulename{td-ok},
\Rulename{l-ok},
\Rulename{nested-ok} and \Rulename{method-ok},
followed by expression typing rules
\Rulename{subsumption}, \Rulename{method-call}, \Rulename{x} and \Rulename{static-method-call}.

Rules \Rulename{cd-ok} and \Rulename{td-ok}
are interesting: a top level class is typed by replacing \Q@This@ with the class name,
and is required to be coherent.
On the other side, a top level trait is typed by temporary adding to the typed program a mapping for
\Q@This@.

\noindent\textbf{Define }$\text{coherent}(\mT,\mL)$\\
$\begin{array}{l}
\!\!\!\bullet\ \text{coherent}(\mT,
\oC \Opt{\terminalCode{interface}}\ \terminalCode{implements} \overline\mT\ \overline\mM\cC
)\quad\text{where}\\

\quad\quad \forall \mC\terminalCode{=}\mL'\in\overline\mM \text{coherent}(\mT\terminalCode{.}\mC,\mL')\\
\quad\quad \text{ either }
\Opt{\terminalCode{interface}}=\terminalCode{interface}\\
\quad\quad \text{or } 
\forall
\terminalCode{method}\ \mT'\ \mm\oR\overline{\mT\,\mx}\cR \in\overline\mM
\text{state}(\text{factory}(\mT,\overline\mM),\mM)
\end{array}$

\noindent A Library is \emph{coherent} if 
all the nested classes are coherent,
and either the Library is an interface,
there are no static methods, or all the static methods
are a valid \emph{state} methods of the candidate \emph{factory}.
Note, by asking for
$\terminalCode{method}\ \mT'\ \mm\oR\overline{\mT\,\mx}\cR \in\overline\mM$
we select only the abstract methods.

\noindent\textbf{Define }$\text{factory}(\mT,\overline\mM)$\\
$\begin{array}{l}

\!\!\!\bullet\ \text{factory}(\mT,\mM_1\ldots\mM_n)=\mM_i=\terminalCode{static method}\ \mT\, \mm
\oR
\_
\cR

\quad\text{where}\\
\quad\quad \forall j\neq i.\ \mM_j=
\text{not of the form}\ \terminalCode{static method}\ \_\, \_
\oR
\_
\cR
\end{array}$

\noindent The candidate factory is the only static abstract  method, and
its return type is the nominal type of our class.

\noindent\textbf{Define }$\text{state}(\mM,\mM')$\\
$\begin{array}{l}


\!\!\!\bullet\ \text{state}(
\terminalCode{static}\ \terminalCode{method}\ \mT\ \mm\oR\mT_1\,\mx_1\ldots\mT_n\,\mx_n\cR,
\terminalCode{method}\ \mT_i\ \mx_i\oR\cR
)\\

\!\!\!\bullet\ \text{state}(
\terminalCode{static} \terminalCode{method}\ \mT\ \mm\oR\mT_1\,\mx_1\ldots\mT_n\,\mx_n\cR,
\terminalCode{method}\ \terminalCode{Void} \mx_i\oR\mT_i\,\terminalCode{that}\cR
)\\

\!\!\!\bullet\ \text{state}(
\terminalCode{static} \terminalCode{method}\ \mT\ \mm\oR\mT_1\,\mx_1\ldots\mT_n\,\mx_n\cR,
\terminalCode{method}\ \mT \terminalCode{with}\mx_i\oR\mT_i\,\terminalCode{that}\cR
)\\

\end{array}$

\noindent A non static method is part of the \emph{abstract state} if 
it s a valid getter, setter or wither.


Rule \Rulename{Nested-OK} helps to accumulate the type of \Q@this@ so that rule \Rulename{Method-OK}
can use it.
The other rules are straightforward and standard.

\subsection{Formal properties}
In addition to conventional soundness of the expression reduction,
\name ensures soundness of the compilation process itself.
A similar property was called meta-level-soundness in~\cite{}; here we can obtain the same result in
a much simpler setting.
We denote with $\text{wrong}(\overline\mD,\mE)$ the count of $\mL$ such that
$\mE=\ctx[\mL]\ \text{and not}\ \overline\mD\vdash\mL:\text{OK}$.

Theorem compilation soundness:

if $\mE_0 \xrightarrow[\smallDs]{} \mE_1$
then $\text{wrong}(\overline\mD,\mE_0)\geq\text{wrong}(\overline\mD,\mE_1)$.

This property has two important corollary:
\begin{itemize}
\item If a class is declared by using $\mC : \use\ \overline\mt$, that is without literals,
and the flattening is successful then \mC\ is well-typed: there is no need of further checking.
\item On the other side, if a class is declared by $\mC : \use\ \overline\mV$, with
$\mL_1\ldots\mL_n \in \overline\mV$, and after successful flattening $\mC : \mL$ can not be type-checked,
then the issue was originally present in one of $\mL_1\ldots\mL_n$.
It may be that the result is intrinsically ill-typed, if one of the methods in $\mL_1\ldots\mL_n$ is not well typed,
but it may also happen that a type referred from one of those methods
is declared \emph{after} the current class. 
In the next section we discuss why we believe this is an acceptable and even useful limitation.

This also means that as an optimization strategy
 we may remember what method bodies come from traits and what method bodies come from code literals, in order to typecheck only the latter.
 \end{itemize}






\subsection{Advantages of our compilation process}


Our typing discipline is very simple from a formal perspective,  
and is what distinguishes our approach from a simple minded code composition macro~\cite{bawden1999quasiquotation}
or a rigid module composition~\cite{ancona2002calculus}. 
It is build on two core ideas:

\paragraph{1: traits are \textbf{well-typed} before being reused.}
 For example in

\saveSpace\begin{lstlisting}
t={method int m() 2 
   method int n() this.m()+1}
\end{lstlisting}\saveSpace

\noindent \Q@t@ is well typed since \Q@m()@ is declared inside of \Q@t@, while

\saveSpace\begin{lstlisting}
t1={method int n() this.m()+1} 
\end{lstlisting}\saveSpace
\noindent would be ill-typed.

\paragraph{2: code literals are \textbf{not required to be well-typed} before flattening.}${}_{}$\\*
In class expressions  $\use\ \overline\mV$,
an \mL\ in $\overline\mV$ is not type-checked before flattening, and only the result is expected to be well-typed.
While this seems a dangerous approach at first, consider that Java also has the same behaviour:
for example in
\saveSpace\begin{lstlisting}[language=Java]
  class A{ int ma() {return 2;}  int n(){return this.ma()+1;} }
  class B extends A{ int mb(){return this.ma();} }
\end{lstlisting}\saveSpace
\noindent in \Q@B@ we can call \lstinline{this.ma()} even if in the curly braces there is no declaration for \Q@ma()@.
In our example, using the trait \Q@t@

\saveSpace\begin{lstlisting}
C: Use t {method int k() this.n()+this.m()}
\end{lstlisting}\saveSpace
\noindent would be correct. In the code literal
\Q@{method int k() this.n()+this.m()}@, 
 even if \Q@n@, \Q@m@ are not locally defined, in 
\name the result of the flattening is well-typed.
This is not the case in many similar works in literature~\cite{deep,Bettini2015282,Bergel2007} where the
literals have to be \emph{self contained}. In this case we would have been forced to
declare abstract methods \Q@n@ and \Q@m@, even if \Q@t@ already 
provides such methods.


Our typing discipline allows recursive types, and
OO languages leverage on recursive types most of the times:
for example \Q@String@ may offer a \Q@Int size()@
method, and \Q@Int@ may offer a \Q@String toString()@ method.
This means that typing classes 
\Q@String@ and \Q@Int@ in isolation one at the time is not possible.


To solve this problem, \textbf{state of the art systems}
supporting recursive types and code reuse/adaptation often accept a great deal of
complication in order to \textbf{predict the structural shape} of the resulting code before doing the actual
code reuse/adaptation~\cite{deepfjig,traitsFerr,packageTempl,MetaML}.
Following those approaches, 
the most expressive compilation process may divide the program in groups of mutually 
dependent classes.
Each group may also depend on a number of other groups.
This would form a direct acyclic graph of groups.
To type a group, they need to type all depended groups, then
to extract the signature/structural shape of all
the classes of the group.
Then, with the information of the depended groups and the one extracted
from the current group, it is possible to typecheck the implementation
 of each class in the group.
In this model, it is reasonable to assume that flattening happens group by group, before
extracting the class signatures.

In our simpler but still sound approach,
we support recursive types between multiple $\mC$\Q@=@$\mE$ \textbf{without
the need of predicting the resulting shape}.%
\footnote{This is a great simplification of our approach,
and is fundamental in the full language where arbitrary code can be run at compile time.}
Indeed we have a much simpler top down execution/interpretation for flattening:
it happens one at the time, and classes are typechecked 
in order and only where their type is first needed,
that is, when they are required to type a trait $\mt$ used in an expression $\mE$.

That is, in \name typing and flattening are interleaved. We assume our compilation process to stop as soon as 
an error arises. 
For example
\saveSpace\begin{lstlisting}
ta:{method int ma() 2}
tc:{method int mc(A a,B b) b.mb(a)}
A: Use ta
B:{method int mb(A a) a.ma()+1}
C: Use tc, {method int hello() 1}
\end{lstlisting}\saveSpace
In this scenario, since we go top down, we first need to generate \Q@A@.
To generate \Q@A@, we need to use \Q@ta@ (but we do not need
\Q@tc@, in rule \Rulename{top} $\mD=$\Q@ta@ and $\mD'=$\Q@tc@).
In order to modularly ensure well typedness,
we require \Q@ta@ to be well typed at this stage. If \Q@ta@ was not well typed
a type error could be generated at this stage.
At this moment, \Q@tc@ cannot be compiled/checked alone:
information about \Q@A@ and \Q@B@ is needed.

Now, we need to generate \Q@C@, and we need to ensure well typedness of \Q@tc@.
Now \Q@A@ is already well typed (since generated by \use\ \Q@ta@, with no \mL),
and \Q@B@ can be typed. Finally \Q@tc@ can be typed and used.
If \use\ could not be performed (for example it \Q@tc@ had a method \Q@hello@ too)
a composition error could be generated at this stage.
On the opposite side, if \Q@B@ and \Q@C@ were swapped, as in
\saveSpace\begin{lstlisting}
C: Use tc, {method int hello() 1}
B:{method int mb(A a) a.ma()+1}
\end{lstlisting}\saveSpace
\noindent
we would be unable to type \Q@tc@, since we need to know the type of \Q@A@ and \Q@B@.
A type error would be generated, on the line of ``flattening of \Q@C@
requires \Q@tc@, \Q@tc@ requires \Q@B@ that is defined later''.

%In this example, a more expressive compilation/precompilation process 
%could compute a dependency graph and, if possible, reorganize the list,

\paragraph{The cost: what expressive power we lose}${}_{}$\\*

For simplicity, we consider to always be able to provide the declarations in the right order, if one exists.
An example of a ``morally correct" program where no right order exists is the following:
\saveSpace\begin{lstlisting}
t={ int mt(A a) a.ma()}
A=Use t {int ma() 1}
\end{lstlisting}\saveSpace

In sharp contrast with
many other approaches
we chose to not support this kind of involved programs.
In a system without inference for method types,
if the result of composition operators depends only on the
structural shape of their input (as for \use)
is indeed possible to optimistically compute the resulting structural shape of the classes
and use it to type involved examples like the former.
We stick to our simple approach, since we believe such typing discipline would be fragile,
and could make human understanding the code-reuse process much harder/involved.
Indeed we just wrote an involved program where the correctness of trait \Q@t@ depends of 
\Q@A@, that is in turn generated using trait \Q@t@.

%Rewriting our example in Java may help to show how involved it is.
%\saveSpace\begin{lstlisting}
%class T{ int mt(A a){return a.ma();}
%class A extends T {int ma() {return 1;}}
%\end{lstlisting}\saveSpace


\paragraph{In \name, typechecking before compiling would be redundant}${}_{}$\\*
In the world of strongly typed languages we are tempted to
first check that all can go well, and then perform the flattening.
This would however be overcomplicated without any observable difference:
Indeed, in the \Q@A,B,C@ example above there is no difference
between
\begin{itemize}
\item  (1)First check \Q@B@ and produce \Q@B@ code (that also contains \Q@B@ structural shape),
  (2) then use \Q@B@ shape to check \Q@C@ and produce \Q@C@ code;\ 
or a more involved
\item  (1)First check \Q@B@ and discover just \Q@B@ structural shape as result of the checking,
  (2)then use \Q@B@ shape to check \Q@C@.
  (3) Finally produce both \Q@B@ and \Q@C@ code.
\end{itemize}

\noindent Note that we can reuse code only by naming traits; but our point of relaxation is {\bf only} the code literal: in no way an error can ``move around'' and be duplicated during the compilation process.
Our approach allows for safe libraries of traits and classes to be typechecked, deployed and reused by multiple clients: no type error will emerge from library code.
However, we do not force the programmer to write self-contained code where all the abstract method definition are explicitly declared.



\subsection{Expression reduction}
Reduction rules are incredibly simple and standard.
A great advantage of our compilation model is that expressions are executed on
a simple fully flattened program, 
where all the composition operators have been removed.
From the point of view of expression reduction, \name is a simple language of 
interfaces and final classes, where nested classes gives structure to the code but have no special semantic.

The reduction of expression is composed by rules
\Rulename{ctx-v},\Rulename{s-m} and \Rulename{m}.
The only interesting point is the auxiliary function meth:


\noindent\textbf{Define }$\text{meth}(\mM,\overline\vds)$

$\begin{array}{l}

\!\!\!\bullet\text{meth}(\terminalCode{static method}\ \mT\ \mm\oR\mT_1\, \mx_1\ldots\mT_n\,\mx_n\cR\me,\vds_1\ldots\vds_n)=\me[\mx_1=\vds_1\ldots\me_n=\vds_n]
\\
\!\!\!\bullet\text{meth}(\terminalCode{method}\ \mT\ \mm\oR\mT_1\, \mx_1\ldots\mT_n\,\mx_n\cR\me,\vds_0\ldots\vds_n)=\me[\terminalCode{this}=\vds_0,\mx_1=\vds_1\ldots\me_n=\vds_n]
\\
\!\!\!\bullet\text{meth}(\terminalCode{method}\ \mT_i\ \mm\oR\cR,\mT\terminalCode{.}\mm\oR\vds_1\ldots\vds_n\cR)=\vds_i\quad\text{where}\\
\quad\quad \overline\mD(\mT,\mm) =
\terminalCode{static method}
\ \mT\,\mm\oR\mT_1\,\mx_1\ldots\mT_n\,\mx_n\cR
\end{array}$

\noindent 
Here we take care of reading bodies and preparing for
execution.
The first case is about static methods,
the second is about instance methods.
The third and fourth cases are more interesting, since they take care of
the abstract state:
the third case takes care of getters and the fourth takes care of withers.
In our formalization we are not modelling state mutation, so there is 
no case for setters.
