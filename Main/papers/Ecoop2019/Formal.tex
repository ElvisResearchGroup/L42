\section{Language grammar and well formedness}
\begin{minipage}{0.63\textwidth}
\begin{bnf}
\production{%
e}      {x \mmid{} e\Q{.}m\rp{es} \mmid{}T\Q{.}m\rp{es}
\mmid{} e\Q{.}x \mmid{} \Q{new} T\rp{es}
}{expression}\\\production{%
L}      {\libi{Tz}{Ms}
%}{interface literal}
%\\\prodNextLine{%
\mmid{} \libc{Tz}{Mz}{K}
}        {code literal}\\\production{%
M}      {\Q{static}$?$ T m\rp{Txs} e$?$ \mmid{} \Q{private}$?$ C\eq{}E }                                                    {member}\\\production{%
K}      {\rp{Txz}$?$}                                          {state}\\\production{%
%CD}     {C\eq{}E}                                                          {class declaration}\\\production{%
%CV}     {C\eq{}LV}                                                         {evaluated class declaration}\\\production{%
%source}      {Ds e}                                                             {source code}\\\production{%
E}      {L \mmid{} t \mmid{} \summ{$E_1$}{$E_2$} \mmid{} \red{E}{Cs}{T}}           {Code Expr.}%
%amt}    {T m\rp{Txs}}                                                      {abstract method}\\\production{%
%mt}     {\Q{static}$?$ T m\rp{Txs} e$?$}                                                 {method}\\\production{%
\end{bnf}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{bnf}
\production{%
T}      {\Q{This}n\Q{.}Cs}                                                 {types}\\\production{%
Tx}     {T x}                                                              {parameter}\\\production{%
D}      {id\eq{}E}                                                         {declaration}\\\production{%
id}     {C \mmid{} t}                                                      {class/trait id}\\\production{%
v}      {\Q{new} T\rp{vs}}                                                 {value}%\\\production{%
\end{bnf}
\end{minipage}


We apply our ideas on a simplified object oriented language with nominal typing and (nested)
interfaces  and final classes.
Instead of inheritance, code reuse is obtained by trait composition, thus the source code would be
a sequence of top level declarations $D$ followed by a main expression;
a lower-case identifier $t$ is a trait name, while an upper case
identifier $C$ is a class name.
To simplify our terminology, instead of distinguishing between 
nested classes and nested interfaces, we will call \emph{nested class} any member of a code literal 
named by a class identifier $C$. Thus, the term \emph{class} may denote either an \emph{interface class} (interface for short) or a \emph{final class}.

In the context of nested classes, types are paths. Syntactically,
we represent them as relative paths of form 
$\This{n}{Cs}$, where the number $n$ identify the root of our path:
\Q@This0@ is the current class, \Q@This1@ is the enclosing class, \Q@This2@ is the enclosing enclosing class and so on. $\This{n}{Cs}$
refers to the class obtained by navigating throughout  $Cs$ starting from $\This{n}$.
Thus, $\This{0}$ is just the type of the directly enclosing class.
By using a larger then needed $n$, there could be multiple different types referring to the same class.
Here we expect all types to be in the normalized form where the smallest possible $n$ is used.

Code literals $L$
serve the role of class/interface bodies; they contain the set of implemented interfaces
$Tz$, the set of members $Mz$ and their (optional) state.
In the concrete syntax we will use \Q@implements@ in front of a non empty list of implemented interfaces
and we will omit parenthesis around a non empty set of fields.
A class member $M$ can be a (private) nested class or a (static) method.
Abstract methods are just methods without a body. 
Well formed interface methods can only be abstract and non-static.
To facilitate code reuse, classes can have (static) abstract methods, code composition is expected to 
provide an implementation for those or, as we will see, redirect away the whole class.
We could easily support private methods too, but to simplify our formalism we consider private only for nested classes. In a well formed code literal, in all types of form $\This{n}{Cs}{C}{Cs'}$,
if $C$ denotes a private nested class, then $Cs$ is empty.


Expressions are used as body of (static) methods and for the main expression.
They are variables $x$ (including \Q@this@)
and conventional (static) method calls.
Field access and \Q@new@ expressions are included but with restricted usage:
well formed field accesses are of form \Q@this.@$x$ in method bodies and
$v$\Q@.@$x$  in the main expression, while 
well formed \Q@new@ expressions have to be of form \Q@new This0(@$xs$\Q@)@ in method bodies
and of form $v$ in the main expression.
Those restrictions greatly simply reasoning about code reuse, since they require different classes to
only communicate by calling (static) methods. Supporting unrestricted fields and constructors would make the formalism much more involved without adding much of a conceptual difficulty.
Values are of form \textit{\Q{new} T\rp{vs}}.

For brevity, in the concrete syntax we assume a syntactic sugar declaring
a static \Q@of@ method (that serve as a factory) and all fields getters; 
thus the order of the fields would induce the order of the factory arguments.
In the core calculus we just assume such methods to be explicitly declared.

Finally, we examine the shape of a nested class: \textit{\Q{private}$?$ C\eq{}E}.
The right hand side is not just a code literal but a code composition expression $E$.
In trait composition, the code expression will be reduced/flattened to a code literal $L$
during compilation.
Code expressions denote an algebra of code composition, starting from code literal $L$
and trait names $t$, referring to a literal declared before by \textit{t\eq{}E}.
We consider two operators: conventional preferential sum 
 \textit{\summ{$E_1$}{$E_2$}} and our novel redirect\textit{\red{E}{Cs}{T}}.

The compilation process consists in flattening all the $E$ into $L$,
starting from the innermost leftmost $E$. This means that sum and redirect work on $LV$s:
a kind of $L$, where all the nested classes are of form \textit{C\eq{}LV}.
The execution happens after compilation and consist in the conventional execution of the main expression $e$
in the context of the fully reduced declarations, where all trait composition has
been flatted away.
Thus, execution is very simple and standard and behaves like a variation of FJ[] with interfaces
instead of inheritance, and where nested classes are just a way to hierarchically organize code names.
On the other side, code composition in this setting is very interesting and powerful, where
nested classes are much more than name organization: they support in a simple and intuitive way
expressive code reuse patterns.
To flatten an $E$ we need to understand the behaviour of the two operators, and how to 
load the code of a trait: since it was written in another place, the syntactic representation of
the types need to be updated.
For each of those points we will first provide some informal explanation and then we will proceed formalizing
the precise behaviour.

\subsection{Redirect}

Redirect %\textit{\red{LV}{$Cs_1:T_1\ldots Cs_n:T_n$}{}}
takes a library literal and produce a modified version of it where some nested classes has been removed
and all the types referencing such nested classes are now referring to an external type. It is easy to use this feature to encode a generic list:

%final class LinkedList<T> {
%  boolean isEmpty(){ return impl == Impl.empty; }
%  T head() { return impl.asCons().elem; }
%  LinkedList tail(){ return impl.asCons().tail; }
%  LinkedList cons(T e){return new LinkedList<T>(){{
%    impl=new Cons(){{elem=e; tail=impl}};}};}
%  private final Supplier<Cons> impl = empty;
%  static final Supplier<Cons> empty = ()->{throw ..;};
%  private static class Cons implements Supplier<Cons>{
%    T elem; Impl tail; Cons get(){return this;}}
%  } 
\begin{lstlisting}
list={
  Elem={}
  static This0 empty()= new This0(Empty.of())
  boolean isEmpty()= this.impl().isEmpty()
  Elem head()= this.impl.asCons().tail()
  This0 tail()=this.impl.asCons().tail()
  This0 cons(Elem e)=new This0(Cons.of(e, this.impl)
  private Impl={interface   Bool isEmpty()  Cons asCons()}
  private Empty={implements This1
    Bool isEmpty()=true  Cons asCons()=../*error*/
    ()}//() means no fields
  private Cons={implements This1
    Bool isEmpty()=false  Cons asCons()=this
    Elem elem Impl tail }
  Impl impl
  }
IntList=list<Elem=Int>
...
IntList.Empty.of().push(3).top()==4 //example usage
\end{lstlisting}
This would flatten into
\begin{lstlisting}
list={/*as before*/
//IntList=list<Elem=Int>
IntList={
  //Elem={} no more nested class Elem
  static This0 empty()= new This0(Empty.of())
  boolean isEmpty()= this.impl().isEmpty()
  Int head()= this.impl.asCons().tail()
  This0 tail()=this.impl.asCons().tail()
  This0 cons(Int e)=new This0(Cons.of(e, this.impl)
  private Impl={interface   Bool isEmpty()  Cons asCons()}
  private Empty={/*as before*/}
  private Cons={implements This1
    Bool isEmpty()=false  Cons asCons()=this
    Int elem Impl tail }
  Impl impl
  }//everywhere there was "Elem", now there is "Int"
\end{lstlisting}

Redirect can be propagated in the same way generics parameters are propagate:
For example, in Java one could write code as below,
\begin{lstlisting}
class ShapeGroup<T extends Shape>{
  List<T> shapes;
  ..}
//alternative implementation
class ShapeGroup<T extends Shape,L extends List<T>>{
  L shapes;
  ..}
\end{lstlisting}
to denote a class containing a list of a certain kind of \Q@Shape@s.
In our approach, one could write the equivalent
\begin{lstlisting}
shapeGroup={
  Shape={implements Shape}
  List=list<Elem=Shape>
  List shapes
  ..}
\end{lstlisting}
With redirect, \Q@shapeGroup@ follow both roles of the two Java examples;
indeed there are two reasonable ways to reuse this code

\Q@Triangolation=shapeGroup<Shape=Triangle>@,
if we have a \Q@Triangle@ class and we would like the concrete list type
used inside to be local to the \Q@Triangolation@,
or 
\Q@Triangolation=shapeGroup<List=Triangles>@,
if we have a preferred implementation for the list of triangles that is going to be used by our
\Q@Triangolation@.
Those two versions would flatten as follow:
\begin{lstlisting}
//Triangolation=shapeGroup<Shape=Triangle>
Triangolation={
  List=/*list with Triangle instead of Elem*/
  List shapes
  ..}

//Triangolation=shapeGroup<List=Triangles>
//exapands to shapeGroup<List=Triangles,Shape=Triangle>
Triangolation={
  Triangles shapes
  ..}
\end{lstlisting}
As you can see, with redirect we do not decide a priori what is generic and what is not in a class.

Redirect can not always succeed. For example, if we was to attempt
\Q@shapeGroup<List=Int>@ the flattening process would fail with an error similar to a 
invalid generic instantiation.
Subtype is a fundamental feature of object oriented programming. Our proposed redirect operator
do not require the type of the target to perfectly match the structural type of the internal nested classes;
structural subtyping is sufficient.
This feature adds a lot of flexibility to our redirect,
however completing the mapping
(as happens in the example above) is a challenging and technically very interesting
task when subtyping is took into account. This is strongly connected with ontology matching and
will be discussed in the technical core of the paper later on.

\subsection{Preferential sum}
The sum of two traits is conceptually a trait with the sum of the traits members, and the union of the implemented interfaces.
If the two traits both define a method with the same name, some resolution strategy is applied.
In the symmetric sum[] the two methods need to have the same signature and at least one of them need to be abstract.
With preferential sum (sometimes called override), if they are both implemented, the left implementation is chosen.
Since in our model we have nested classes, nested classes with the same name will be recursively composed.

We chose preferential sum since is simpler to use in short code examples.~\footnote{symmetric sum is often presented in conjunction with a restrict operator that makes some methods abstract.}
Since the focus of the paper is the novel redirect operator, instead of the well known sum, we will handle summing state and interfaces in the simplest possible way:
a class with state can only be summed with a class without state, and an interface can only be summed with another interface with identical methods signatures.

In literature it has been shown how trait composition with (recursively composed) nested classes can 
elegantly handle the expression problem and a range of similar design challenges.
Here we will show some examples where sum and redirect cooperate to produce interesting code reuse patterns:

\begin{lstlisting}
listComp=list<+{
  Elem:{ Int geq(Elem e)}//-1/0/1 for smaller, equals, greater
  static Elem max2(Elem e1, Elem e2)=if e1.geq(e2)>0 then e1, else e2
  Elem max(Elem candidate)=
    if This.isEmpty() then candidate
    else this.tail().max(This.max2(this.head(),candidate))
  Elem min(Elem candidate)=...
  This0 sort()=...
  }
\end{lstlisting}
As you can see, we can \emph{extends} our generic type while refining our generic argument:
\Q@Elem@ of \Q@listComp@ now needs a \Q@geq@ method.

While this is also possible with conventional inheritance and F-Bound polymorphism, we think this solution is logically simpler then the equivalent Java
\begin{lstlisting}
class ListComp<Elem extends Comparable<Elem>> extends LinkedList<Elem>{
  ../*body as before*/
  }
\end{lstlisting}

In the ending of this paper we will show how redirect and sum allows to encode
difficult code reuse patterns in a much more convenient way that Java, Scala or Rust.


\begin{bnf}
\production{%
\ctx{V}}{\hole \mmid{}  \summ{\ctx{V}}{E} %
                \mmid{}  \summ{LV}{\ctx{V}} \mmid{} \red{\ctx{V}}{Cs}{T}}  {context of library-evaluation}\\\production{%
LV}     {\libi{Tz}{amtz}{}\ \ \mmid{}\ \ \libc{Tz}{MVs}{K$?$}}       {literal value}\\\production{%
MV}     {C\eq{}LV \mmid{} mt}                                                    {}\\\production{%
\ctx{v}}{\hole \mmid{}  \ctx{v}\Q{.}m\rp{es} \mmid{}  v\Q{.}m\rp{vs \ctx{v} es} %
	\mmid{} T\Q{.}m\rp{vs \ctx{v} es}  }           {}\\\production{%
DL}     {id\eq{}L}                                                         {partially-evaluated-declaration}\\\production{%
DV}     {id\eq{}LV}                                                       {evaluated-declaration}\\\production{%
Mid}    {C \mmid{} m}                                                      {member-id}\\\production{%
p}      {DLs\Q{;} DVs}                                                     {program}
\end{bnf}

We use $t$ and $C$ to syntactically distinguish between trait and class names.
An $E$ is a top-level class expression, which can contain class-literals, references to traits, and operations on them, namely our sum $E <+ E$ and redirect $e(Cs=T)$.
A declaration $D$ is just an $id = E$, representing that $id$ is declared to be the value of $E$, we also have $CD, CV, DL$, and $DV$ that constrain the forms of the LHS and RHS of the declaration.
A literal $L$ has 4 components, an optional interface keyword, a list of implemented interfaces, a list of members, and an optional constructor. For simplicity, interfaces can only contain abstract-methods ($amt$) as members, and cannot have  constructors. A member $M$, is either an (potentially abstract) methood $mt$ or a nested class declaration $(CD)$. A member value $MV$, is a member that has been fully compiled. An $mid$ is an identifier, identifying a member.
Constructors, $K$, contain a $Txs$ indicating the type and names of fields. An $e$ is normal fetherweight-java style expression, it has variables $x$, method calls $e.m(es)$, field accesses $e.x$ and object creation $new es$.
$CtxV$ is the evalation context for class-expressions $E$, and $ctxv$ is the usuall one for $e$’s.

An $S$ represents what the top-level source-code form of our language is, it’s just a sequence of declarations and a main expression.
The most interesting form of the grammer is a $p$, it is a ’program’, used as the context for many reductions and typing rules, on the LHS of the $;$ is a stack representing which (nested) declaration is currently being processed, the bottom (rightmost) $DL$ represents the $D$ of the source-program that is currently being processed. Th RHS of the $;$ represents the top-level declarations that have allready been compiled, this is neccessary to look up top-level classes and traits.

\noindent To look up the value of a type in the program we will use the notation $p(T)$, which is defined by the following, but only if the RHS denotes an $LV$:
\begin{defye}%
\defy{(; \_, C\eq{}L, \_)(\This{0}{C}{Cs})}{\mathit{L(Cs)}}%
\defy{(id\eq{}L, p)(\This{0}{Cs})}{L(\mathit{Cs})}%
%p' id=L,A;B = id=L,p
\defy{(id\eq{}L, p)(\This{n+1}{Cs})}{p(\This{n}{Cs})}%
\end{defye}

\noindent To get the relative value of a trait, we define $p[t]$:
\begin{defye}%
\defy{(\s{DL}; \_, t\eq{LV},\_)[t]}{\from{LV}{\This{\#DLs}}}
\end{defye}

\noindent To get a the value of a literal, in a way that can be understand from the current location (\This{0}), we define:
\begin{defye}%
	\defy{p[T]}{\from{p(T)}{T}}%
\end{defye}

\noindent And a few simple auxiliary definitions:
\begin{defye}%
	\defy{\s{T} \in p}{\forall T \in \s{T} \bullet p(T) \text{ is defined}}%
	\defy{L({\emptyset})}{L}%
	\defy{L(\Cs{C}{\s{C}})}{L(Cs) \text{ where } L = \lib{\_}{\_, C\eq{L}, \_}{\_}}%
	\defy{L[C\eq{E'}]}{\lib{Tz}{\s{MV}\ C\eq{E'}\ \s{M}}{K?}}
	\defyc{\text{where } L = \lib{Tz}{\s{MV}\ C\eq{\_}\ \s{M}}{K?}}
\end{defye}

\newpage

\begin{comment}
Define p(P) = LV
-----------------------------------


p.exists(Ps) iff forall P in Ps: p(P) is defined

Define L(Cs) = L
--------------------------------------
L(empty) = L
L(C.Cs) = L(Cs)
    L = interface? {_; _, C = L, _; _}

Define L[CD]=L'
---------------------------------------------
L[C = E'] = interface? {Tz; MVs C = E' Ms; K?}
  L = interface? {Tz; MVs C = _ Ms; K?}


Define operations on p
--------------------------------------
p.evilPush(L) = (C = L, p)
	for fresh C

p.push(id) = (id = L, p)
    p = (id' = {_;_, id = L, _ ;_}, _; Ds)

(id = L, p).pop() = p
(id = L, p).top() = L

Define equivy ops...
------------------------------
empty =p empty
P, Ps =p P', Ps' iff:
	p.minimize(P) = p.minimize(P')
	Ps =p Ps'

Pz subseteq_p Pz' iff:
	p.minimize(Pz) subseteq p.minimize(Pz')

p.minimize(empty) = empty
p.minimize(P, Pz) = p.minimize(P), p.minimize(Pz)

p.minimize(Thisn+1.idn.Cs) = p.minimize(Thisn.Cs):
  p = id0 = L0, ..., idn = Ln, _; Ds
  p(Thisn.Cs) = L
  // TODO: Check that Ln is an L instead?

otherwise p.minimize(P) = P

define dom(Mz) = Midz
===========================================
dom(empty) = empty
dom(C = E, Mz) = C, dom(Mz)
dom(T m(Txs), Mz) = m, dom(Mz)
\end{comment}

We have two-top level reduction rules defining our language, of the form $Ds e ––> Ds’ e$ which simply reduces the source-code.
The first rule $(compile)$ ‘compiles’ each top-level declaration (using a well-typed subset of allready compiled top-level declarations), this reduces the defining expresion.
The second rule, $(main)$ is executed once all the top-level declarations have compiled (i.e. are now fully evaluated class literals), it typechecks the top-level declarations and the main expression, and then procedes to reduce it.
In principle only one-typechecking is needed, but we repeat it to avoid declaring more rules.
\begin{verbatim}
Define Ds e --> Ds' e'
================================================================
DVs' |- Ok
empty; DVs'; id | E --> E'
(compile)---------------------------------------- DVs' subsetof DVs
DVs id = E Ds e --> DVs id = E' Ds e

DVs |- Ok
DVs |- e : T
DVs |- e --> e'
(main)---------------------------------- for some type T
DVs e --> DVs e'
\end{verbatim}



\section{Compilation}

Aside from the redirect operation itself, compilation is the most interesting part, it is defined by a reduction arrow $p; id |- E --> E’$, the $id$ represents the id of the type/trait that we are currently compiling, it is needed since it will be the name of $This0$, and we use that fact that that is equal to $This1.id$ to compare types for equality.
The $(CtxV)$ rule is the standard context, the $(L)$ rule propegates compilation inside of nested-classes, $(trait)$ merely evaluates a trait reference to it’s defined body, $(sum)$ and $(redirect)$ perform our two meta-operations.

\begin{verbatim}
Define p; id |- E --> E'
=========================================================
p; id |- E --> E'
(CtxV) ----------------------------------------------
p; id |- CtxV[E] --> CtxV[E']

id = L[C = E], p; C |- E --> E'
(L) --------------------------------------------- // TODO use fresh C?
p; id |- L[C = E] ---> L[C = E']

(trait) -----------------------------------
p; id |- t -> p[t]

LV1 <+p' LV2 = LV3                  p' = C' = LV3, p
(sum) -------------------------------------- for fresh C'
p; id |- LV1 <+ LV2 --> LV3

// TODO: Inline and de-42 redirect formalism
(redirect) ------------------------------------LV'=redirect(p, LV, Cs, P)
p; id |- LV(Cs=P) -> LV'
\end{verbatim}

\section{The Sum operation}

The sum operation is defined by the rule $L1 <+p L2 = L3$, it is unconventional as it assumes we allready have the result ($L3$), and simply checks that it is indead correct.
We believe (but have not proved) that this rule is unambigouse, if $L1 <+ p L2 = L3$ and $L1 <+ p L2 = L3’$, then $L3 = L3’$ (since the order of members does not matter for $L$s).

The main rule fir summong of non-interfaces, sums the members, unions the implemented interfaces (and uses $mininize$ to remove any duplicates), it also ensures that at most one of them has a constructor.
For summing an interface with a interface/class we require that an interface cannot ’gain’ members due to a sum. The actually L42 implementation is far less restrictive, but requires complicated rules to ensure soudness, due to problems that could arise if a summed nested-interface is implemented.
Summing of traits/classes with state is a non-trivial problem and not the focus of our paper, their are many prior works on this topic, and our full L42 language simply uses ordinary methods to represent state, however this would take too much effort to explain here.


\begin{verbatim}
Define L1 <+p L2 = L3
========================================================================================
{Tz1; Mz1; K?1} <+p {Tz2; Mz2; K?2} = {Tz; Mz; K?}
Tz = p.minimize(Tz1 U Tz2)
Mz1 <+p Mz1 = Mz
{empty, K?1, K?2} = {empty, K?} //may be too sophisticated?

interface{Tz1; amtz,amtz';} <+p interface?{Tz2;amtz;} = interface {Tz;amtz,amtz';}
Tz = p.minimize(Tz1 U Tz2)
if interface? = interface then amtz'=empty
\end{verbatim}

The rules for summing member are simple, we take two sets of members collect all the oness with unique names, and sum those with duplicates.
To sum nested classes we merely sum their bodies, to sum two methods we require their signatures to be identical, if they both have bodies, the result has the body of the RHS, otherwise the result has the body (if present) of the LHS.
\begin{verbatim}
Define Mz <+p Mz' = Mz"
------------------------------------------
M, Mz <+p M', Mz' = M <+p M', Mz <+p Mz
//note: only defined when M.Mid = M'.Mid

Mz <+p Mz' = Mz, Mz':
dom(Mz) disjoint dom(Mz')

Define M <+p M' = M"
-----------------------------------------
T' m(Txs') e? <+p T m(Txs) e = T m(Txs) e
T', Txs'.Ts =p Ts, Txs

T' m(Txs') e? <+p T m(Txs) = T m(Txs) e?
T', Txs'.Ts =p Ts, Txs

(C = L) <+p (C = L') = L <+p.push(C) L'
\end{verbatim}

\section{Type System}

The type system is split into two parts: type checking programs and class literals, and the typechecking of expressions. The latter part is mostly convential, it involves typing judgments of the form $p; Txs \vdash e : T$, with the usual program $p$ and variable environement $Txs$ (often called $\Gamma$ in the literature). rule ($Ds ok$) type checks a sequence of top-level declarations by simply push each declaration onto a program and typecheck the resulting program.
Rule $p ok$ typechecks a program by check the topmost class literal: we type check each of it’s members (including all nested classes), check that it properly implements each interface it claims to, does something weird, and finanly check check that it’s constructor only referenced existing types,

\begin{verbatim}


Define p |- Ok
===========================================================

D1; Ds |- Ok ... Dn; Ds|- Ok
(Ds ok) ------------------------------ Ds = D1 ... Dn
Ds |- Ok

p |- M1 : Ok .... p |- Mn : Ok
p |- P1 : Implemented .... p |- Pn : Implemented
p |- implements(Pz; Ms) /*WTF?*/                   if K? = K: p.exists(K.Txs.Ts)
(p ok) ------------------------------------------- p.top() = interface? {P1...Pn; M1, ..., Mn; K?}
p |- Ok

p.minimize(Pz) subseteq p.minimize(p.top().Pz)
amt1 _ in p.top().Ms ... amtn _ in p.top().Ms
(P implemented) ----------------------------------------------- p[P] = interface {Pz; amt1 ... amtn;}
p |- P : Implemented

(amt-ok) ------------------- p.exists(T, Txs.Ts)
p |- T m(Tcs) : Ok

p; This0 this, Txs |- e : T
(mt-ok) ------------------------------ p.exists(T, Txs.Ts)
p |- T m(Tcs) e : Ok

C = L, p |- Ok
(cd-Ok) -------------------
p |- C = L : OK

\end{verbatim}

Rule $(P implemented)$ checks that an interface is properly implemented by the program-top, we simply check that it declares that it implements every one of the interfaces super-interfaces and methods.
Rules $(amt-ok)$ and $(mt-ok)$ are straightforward, they both check that types mensioned in the method signature exist, and ofcourse for the latter case, that the body respects this signature.

To typecheck a nested class declaration, we simply push it onto the program and typecheck the top-of the program as before.


The expression typesystem is mostly straightforward and similar to feartherwieght Java, notable we we use $p[T]$ to look up information about types, as it properly ‘from’s paths, and use a classes constructor definitions to determine the types of fields.

\begin{verbatim}
Define p; Txs |- e : T
=====================================
(var)
----------------------- T x in Txs
p;  Txs |- x : T

(call)
p; Txs |- e0 : T0
...
p; Txs |- en : Tn
-----------------------------------  T' m(T1 x1 ... Tn xn) _ in p[T0].Ms
p; Txs |- e0.m(e1 ... en) : T'

(field)
p; Txs |- e : T
---------------------------------------  p[T].K = constructor(_ T' x _)
p; Txs |- e.x : T'


(new)
p; Txs |- e1 : T1 ... p; Txs |- en : Tn
------------------------------------------- p[T].K = constructor(T1 x1 ... Tn xn)
p; Txs |- new T(e1 ... en)


(sub)
p; Txs |- e : T
-----------------------------------  T' in p[T].Pz
p; Txs |- e : T'


(equiv)
p; Txs |- e : T
-----------------------------------  T =p T'
p; Txs |- e : T'
\end{verbatim}