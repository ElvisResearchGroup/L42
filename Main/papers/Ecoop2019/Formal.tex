\section{Language grammar and well formedness}
We apply our ideas on a simplified object oriented language with nominal typing and (nested)
interfaces  and final classes.
Instead of inheritance, code reuse is obtained by trait composition, thus the source code would be
a sequence of top level declarations $D$ followed by a main expression;
a lower-case identifier $t$ is a trait name, while an upper case
identifier $C$ is a class name.
To simplify our terminology, instead of distinguishing between 
nested classes and nested interfaces, we will call \emph{nested class} any member of a code literal 
named by a class identifier $C$. Thus, the term \emph{class} may denote either an \emph{interface class} (interface for short) or a \emph{final class}.

\begin{minipage}{0.63\textwidth}
\begin{bnf}
\production{%
e}      {x \mmid{} e\Q{.}m\rp{es} \mmid{}T\Q{.}m\rp{es}
\mmid{} e\Q{.}x \mmid{} \Q{new} T\rp{es}
}{expression}\\\production{%
L}      {\libi{Tz}{Ms}
%}{interface literal}
%\\\prodNextLine{%
\mmid{} \libc{Tz}{Mz}{K}
}        {code literal}\\\production{%
M}      {\Q{static}$?$ T m\rp{Txs} e$?$ \mmid{} \Q{private}$?$ C\eq{}E }                                                    {member}\\\production{%
K}      {\rp{Txz}$?$}                                          {state}\\\production{%
%CD}     {C\eq{}E}                                                          {class declaration}\\\production{%
%CV}     {C\eq{}LV}                                                         {evaluated class declaration}\\\production{%
%source}      {Ds e}                                                             {source code}\\\production{%
E}      {L \mmid{} t \mmid{} \summ{$E_1$}{$E_2$} \mmid{} \red{E}{Cs}{T}}           {Code Expr.}%
%amt}    {T m\rp{Txs}}                                                      {abstract method}\\\production{%
%mt}     {\Q{static}$?$ T m\rp{Txs} e$?$}                                                 {method}\\\production{%
\end{bnf}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{bnf}
\production{%
T}      {\Q{This}n\Q{.}Cs}                                                 {types}\\\production{%
Tx}     {T x}                                                              {parameter}\\\production{%
D}      {id\eq{}E}                                                         {declaration}\\\production{%
id}     {C \mmid{} t}                                                      {class/trait id}\\\production{%
v}      {\Q{new} T\rp{vs}}                                                 {value}%\\\production{%
\end{bnf}
\end{minipage}


In the context of nested classes, types are paths. Syntactically,
we represent them as relative paths of form 
$\This{n}{Cs}$, where the number $n$ identify the root of our path:
\Q@This0@ is the current class, \Q@This1@ is the enclosing class, \Q@This2@ is the enclosing enclosing class and so on. $\This{n}{Cs}$
refers to the class obtained by navigating throughout  $Cs$ starting from $\This{n}$.
Thus, $\This{0}$ is just the type of the directly enclosing class.
By using a larger then needed $n$, there could be multiple different types referring to the same class.
Here we expect all types to be in the normalized form where the smallest possible $n$ is used.

Code literals $L$
serve the role of class/interface bodies; they contain the set of implemented interfaces
$Tz$, the set of members $Mz$ and their (optional) state.
In the concrete syntax we will use \Q@implements@ in front of a non empty list of implemented interfaces
and we will omit parenthesis around a non empty set of fields.
To simplifiy our formalism, we delegate some sanity checks well formedness, and we assume
all the fields in the state $K$ to have different names;
no two methods or nested classes with the same name ($m$ or $C$) are declared in a code literal,
and no nested class is named $\This{n}$ for any number $n$;
in any method headers, all parameters have different names, and no parameter is named \Q@this@.

A class member $M$ can be a (private) nested class or a (static) method.
Abstract methods are just methods without a body. 
Well formed interface methods can only be abstract and non-static.
To facilitate code reuse, classes can have (static) abstract methods, code composition is expected to 
provide an implementation for those or, as we will see, redirect away the whole class.
We could easily support private methods too, but to simplify our formalism we consider private only for nested classes. In a well formed code literal, in all types of form $\This{n}{Cs}{C}{Cs'}$,
if $C$ denotes a private nested class, then $Cs$ is empty.
We assume a form of alpha-reaming for private nested classes, that will consistently rename all the 
paths of form $\This{n}{C}{Cs'}$, where 
$\This{n}{C}$ refer to such private nested class. The trivial definition of such alpha rename is given in appendix.

Expressions are used as body of (static) methods and for the main expression.
They are variables $x$ (including \Q@this@)
and conventional (static) method calls.
Field access and \Q@new@ expressions are included but with restricted usage:
well formed field accesses are of form \Q@this.@$x$ in method bodies and
$v$\Q@.@$x$  in the main expression, while 
well formed \Q@new@ expressions have to be of form \Q@new This0(@$xs$\Q@)@ in method bodies
and of form $v$ in the main expression.
Those restrictions greatly simply reasoning about code reuse, since they require different classes to
only communicate by calling (static) methods. Supporting unrestricted fields and constructors would make the formalism much more involved without adding much of a conceptual difficulty.
Values are of form \textit{\Q{new} T\rp{vs}}.

For brevity, in the concrete syntax we assume a syntactic sugar declaring
a static \Q@of@ method (that serve as a factory) and all fields getters; 
thus the order of the fields would induce the order of the factory arguments.
In the core calculus we just assume such methods to be explicitly declared.

Finally, we examine the shape of a nested class: \textit{\Q{private}$?$ C\eq{}E}.
The right hand side is not just a code literal but a code composition expression $E$.
In trait composition, the code expression will be reduced/flattened to a code literal $L$
during compilation.
Code expressions denote an algebra of code composition, starting from code literal $L$
and trait names $t$, referring to a literal declared before by \textit{t\eq{}E}.
We consider two operators: conventional preferential sum 
 \textit{\summ{$E_1$}{$E_2$}} and our novel redirect\textit{\red{E}{Cs}{T}}.

\subsection{Compilation process/flattening}
The compilation process consists in flattening all the $E$ into $L$,
starting from the innermost leftmost $E$. This means that sum and redirect work on $LV$s:
a kind of $L$, where all the nested classes are of form \textit{C\eq{}LV}.
The execution happens after compilation and consist in the conventional execution of the main expression $e$
in the context of the fully reduced declarations, where all trait composition has
been flatted away.
Thus, execution is very simple and standard and behaves like a variation of FJ[] with interfaces
instead of inheritance, and where nested classes are just a way to hierarchically organize code names.
On the other side, code composition in this setting is very interesting and powerful, where
nested classes are much more than name organization: they support in a simple and intuitive way
expressive code reuse patterns.
To flatten an $E$ we need to understand the behaviour of the two operators, and how to 
load the code of a trait: since it was written in another place, the syntactic representation of
the types need to be updated.
For each of those points we will first provide some informal explanation and then we will proceed formalizing
the precise behaviour.

\subsubsection{Redirect}

Redirect %\textit{\red{LV}{$Cs_1:T_1\ldots Cs_n:T_n$}{}}
takes a library literal and produce a modified version of it where some nested classes has been removed
and all the types referencing such nested classes are now referring to an external type. It is easy to use this feature to encode a generic list:

%final class LinkedList<T> {
%  boolean isEmpty(){ return impl == Impl.empty; }
%  T head() { return impl.asCons().elem; }
%  LinkedList tail(){ return impl.asCons().tail; }
%  LinkedList cons(T e){return new LinkedList<T>(){{
%    impl=new Cons(){{elem=e; tail=impl}};}};}
%  private final Supplier<Cons> impl = empty;
%  static final Supplier<Cons> empty = ()->{throw ..;};
%  private static class Cons implements Supplier<Cons>{
%    T elem; Impl tail; Cons get(){return this;}}
%  } 
\begin{lstlisting}
list={
  Elem={}
  static This0 empty()= new This0(Empty.of())
  boolean isEmpty()= this.impl().isEmpty()
  Elem head()= this.impl.asCons().tail()
  This0 tail()=this.impl.asCons().tail()
  This0 cons(Elem e)=new This0(Cons.of(e, this.impl)
  private Impl={interface   Bool isEmpty()  Cons asCons()}
  private Empty={implements This1
    Bool isEmpty()=true  Cons asCons()=../*error*/
    ()}//() means no fields
  private Cons={implements This1
    Bool isEmpty()=false  Cons asCons()=this
    Elem elem Impl tail }
  Impl impl
  }
IntList=list<Elem=Int>
...
IntList.Empty.of().push(3).top()==4 //example usage
\end{lstlisting}
This would flatten into
\begin{lstlisting}
list={/*as before*/
//IntList=list<Elem=Int>
IntList={
  //Elem={} no more nested class Elem
  static This0 empty()= new This0(Empty.of())
  boolean isEmpty()= this.impl().isEmpty()
  Int head()= this.impl.asCons().tail()
  This0 tail()=this.impl.asCons().tail()
  This0 cons(Int e)=new This0(Cons.of(e, this.impl)
  private Impl={interface   Bool isEmpty()  Cons asCons()}
  private Empty={/*as before*/}
  private Cons={implements This1
    Bool isEmpty()=false  Cons asCons()=this
    Int elem Impl tail }
  Impl impl
  }//everywhere there was "Elem", now there is "Int"
\end{lstlisting}

Redirect can be propagated in the same way generics parameters are propagate:
For example, in Java one could write code as below,
\begin{lstlisting}
class ShapeGroup<T extends Shape>{
  List<T> shapes;
  ..}
//alternative implementation
class ShapeGroup<T extends Shape,L extends List<T>>{
  L shapes;
  ..}
\end{lstlisting}
to denote a class containing a list of a certain kind of \Q@Shape@s.
In our approach, one could write the equivalent
\begin{lstlisting}
shapeGroup={
  Shape={implements Shape}
  List=list<Elem=Shape>
  List shapes
  ..}
\end{lstlisting}
With redirect, \Q@shapeGroup@ follow both roles of the two Java examples;
indeed there are two reasonable ways to reuse this code

\Q@Triangolation=shapeGroup<Shape=Triangle>@,
if we have a \Q@Triangle@ class and we would like the concrete list type
used inside to be local to the \Q@Triangolation@,
or 
\Q@Triangolation=shapeGroup<List=Triangles>@,
if we have a preferred implementation for the list of triangles that is going to be used by our
\Q@Triangolation@.
Those two versions would flatten as follow:
\begin{lstlisting}
//Triangolation=shapeGroup<Shape=Triangle>
Triangolation={
  List=/*list with Triangle instead of Elem*/
  List shapes
  ..}

//Triangolation=shapeGroup<List=Triangles>
//exapands to shapeGroup<List=Triangles,Shape=Triangle>
Triangolation={
  Triangles shapes
  ..}
\end{lstlisting}
As you can see, with redirect we do not decide a priori what is generic and what is not in a class.

Redirect can not always succeed. For example, if we was to attempt
\Q@shapeGroup<List=Int>@ the flattening process would fail with an error similar to a 
invalid generic instantiation.
Subtype is a fundamental feature of object oriented programming. Our proposed redirect operator
do not require the type of the target to perfectly match the structural type of the internal nested classes;
structural subtyping is sufficient.
This feature adds a lot of flexibility to our redirect,
however completing the mapping
(as happens in the example above) is a challenging and technically very interesting
task when subtyping is took into account. This is strongly connected with ontology matching and
will be discussed in the technical core of the paper later on.

\subsubsection{Preferential sum and examples of sum and redirect working together}
The sum of two traits is conceptually a trait with the sum of the traits members, and the union of the implemented interfaces.
If the two traits both define a method with the same name, some resolution strategy is applied.
In the symmetric sum[] the two methods need to have the same signature and at least one of them need to be abstract.
With preferential sum (sometimes called override), if they are both implemented, the left implementation is chosen.
Since in our model we have nested classes, nested classes with the same name will be recursively composed.

We chose preferential sum since is simpler to use in short code examples.~\footnote{symmetric sum is often presented in conjunction with a restrict operator that makes some methods abstract.}
Since the focus of the paper is the novel redirect operator, instead of the well known sum, we will handle summing state and interfaces in the simplest possible way:
a class with state can only be summed with a class without state, and an interface can only be summed with another interface with identical methods signatures.

In literature it has been shown how trait composition with (recursively composed) nested classes can 
elegantly handle the expression problem and a range of similar design challenges.
Here we will show some examples where sum and redirect cooperate to produce interesting code reuse patterns:

\begin{lstlisting}
listComp=list<+{
  Elem:{ Int geq(Elem e)}//-1/0/1 for smaller, equals, greater
  static Elem max2(Elem e1, Elem e2)=if e1.geq(e2)>0 then e1, else e2
  Elem max(Elem candidate)=
    if This.isEmpty() then candidate
    else this.tail().max(This.max2(this.head(),candidate))
  Elem min(Elem candidate)=...
  This0 sort()=...
  }
\end{lstlisting}
As you can see, we can \emph{extends} our generic type while refining our generic argument:
\Q@Elem@ of \Q@listComp@ now needs a \Q@geq@ method.

While this is also possible with conventional inheritance and F-Bound polymorphism, we think this solution is logically simpler then the equivalent Java
\begin{lstlisting}
class ListComp<Elem extends Comparable<Elem>> extends LinkedList<Elem>{
  ../*body as before*/
  }
\end{lstlisting}

Another interesting way to use sum is to modularize behaviour delegation: consider the following
(not efficient for the sake of compactness) implementation of \Q@set@, where the way to compare elements is not fixed:
\begin{lstlisting}
set:{
  Elem:{}
  List=list<Elem=Elem>
  static This0 empty()= new This0(List.empty())
  Bool contains(Elem e)=../*uses eq and hash*/
  Int size()=..
  This add(Elem e)=...
  This remove(Elem e)=...
  Bool eq(Elem e1,Elem e2)//abstract
  Int hash(Elem e)//abstract
  List asList //to allow iteration
  }
eqElem={
  Elem={ Bool equals(Elem e)/*abstract*/}
  Bool eq(Elem e1,Elem e2)=e1.equals(e2)
  }
hashElem={
  Elem={ Int hash(Elem e)/*abstract*/}
  Int hash(Elem e)=e.hash()
  }
Strings=(set<+eqElem<+eqHash)<Elem=String>
LongStrings=(set<+eqElem)<Elem=String> <+{
  Int hash(String e)=e.size()
  }//for very long strings, size is a faster hash
\end{lstlisting}
Note how 
\Q@(set<+eqElem<+eqHash)<Elem=String>@
is equivalent to \Q@set<Elem=String> <+eqElem<Elem=String> <+eqHash<Elem=String>@.
Consider now the signature \Q@Bool equals(Elem e)@.
This is different from the common signature \Q@Bool equals(Object e)@. What is the best
signature for \Q@equals@ is an open research question, where most approaches advise either the
first or the second one. Our \Q@eqElem@, as is wrote, can support both:
\Q@Strings@ would be correctly define both if \Q@String.equals@ signature
has a \Q@String@ or an \Q@Object@ parameter.EXPAND on method subtyping.


\subsection{Moving traits around in the program}
It is not trivial to formalize the way types like \Q@This1.A.B@ %$\This{3}{\Q@A@}{\Q@B@}$
have to be adapted so that when code is moved around in different depths of nesting the 
refereed classes stay the same.
This is needed during flattening, when a trait $t$ is reused, but also during reduction, when a method body is inlined in the main expression, and during typing, where a method body is typed depending on the signature of other methods in the system.

To this aim we define a concept of program $p$, as a representation of 
the code as seen from a certain point inside of the source code. It is the most interesting form of the grammar,
used for virtually all reduction and typing rules. On the left of the $;$ is a stack representing which (nested) declaration is currently being processed, the bottom (rightmost) \textit{DL} represents the $D$ of the source-program that is currently being processed. Th right of the $;$ represents the top-level declarations that have already been compiled, this is necessary to look up top-level classes and traits.
That is, each of the $\textit{DL}_0\ldots\textit{DL}_n$
represents the outer nested level $0..n$, while
the \textit{DVs} component represent the already flattened portion of the program top level, that is 
the outer nested level $n+1$

\begin{bnf}
\production{%
p}      {DLs\Q{;} DVs}                                                     {program}\\\production{%
DL}     {id\eq{}L}                                                         {partially-evaluated-declaration}\\\production{%
DV}     {id\eq{}LV}                                                       {evaluated-declaration}\\\production{%
Mid}    {C \mmid{} m}                                                      {member-id}%\\\production{%
\end{bnf}


Thus, for example in the program
\begin{lstlisting}
A={()}
t={ B={()}   This1.A m(This0.B b)}
C={D={E=t}}
H=t<B=A>
\end{lstlisting}
the flattened version for \Q@C.D.E@ will be 
\Q@{ B={()}   This3.A m(This0.B b)}@, where the path
\Q@This1.A@ is now \Q@This3.A@ while the path \Q@This0.B@ stays the same: types defined internally will
stay untouched.
The program $p$ in the observation point \Q@E=t@ is
\begin{lstlisting}
A={()}
t={ B={()}   This1.A m(This0.B b)}
C={D={E=t}};
C={D={E=t}},//this means, we entered in C
D={E=t}//this means, we entered in D
\end{lstlisting}

%We will use $p$ as a function, so we will write
%.....p(T) and p(T.m)
%we define now From,
%....

To fetch a trait form a program, we will use notation $p(t)=LV$, to 
fetch a class we will use $p(T)$.
%We will use $p$ as a stack, where 

%An $S$ represents what the top-level source-code form of our language is, it’s just a sequence of declarations and a main expression.
\noindent To look up the definition of a class in the program we will use the notation
%$p(t)=LV$ and
$p(T)=\textit{LV}$, which is defined by the following:,% but only if the RHS denotes an $LV$:
\begin{defye}%
%\defy{(\_; \_, t\eq{}LV, \_)(t)}{\mathit{LV}}%
\defy{(; \_, C\eq{}L, \_)(\This{0}{C}{Cs})}{\mathit{L(Cs)}}%
%\defy{(id\eq{}L, p)(\This{0}{Cs})}{L(\mathit{Cs})}%
\defy{(id\eq{}L, DLs\Q{;} DVs)(\This{0}{Cs})}{L(\mathit{Cs})}%
%p' id=L,A;B = id=L,p
%\defy{(id\eq{}L, p)(\This{n+1}{Cs})}{p(\This{n}{Cs})}%shorter version, not wrong but confusing
\defy{(id\eq{}L, DLs\Q{;} DVs)(\This{n+1}{Cs})}{DLs\Q{;} DVs(\This{n}{Cs})}%
\defy{LV({\emptyset})}{LV}%
\defy{L(\Cs{C}{\s{C}})}{L_0(Cs) \text{ where } L = \lib{\_}{\_, C\eq{L_0}, \_}{\_}}%
\end{defye}


This notation just fetch the referred $LV$ without any modification.
To adapt the paths we define $\from{T_0}{T_1}$, $\from{L}{T}$ and $p\op{minimize}{T}$ as following:
\begin{defye}%
	\defy{
\from{\This{n}{Cs}}{\This{m}{C_1\ldots C_k}}
}{
\This{m}{C_1\ldots C_{(k-n)}} \quad \textit{with }n\leq k
}
\defy{
\from{\This{n}{Cs}}{\This{m}{C_1\ldots C_k}}
}{
\This{(m+n-k)}{C_1\ldots C_{(k-n)}{Cs}} \quad \textit{with }n> k
}
	\defy{
\from{L}{T}
}{
\from{L}{T}_0
}
\defy{
\from{
\libc{\Q@interface@? Tz}{Mz}{K}
}{T}_j
}{
\libc{\Q@interface@? \from{Tz}{T}_{j+1}}{\from{Mz}{T}_{j+1}}{\from{K}{T}_{j+1}}
}
\defy{
\from{\This{(j+n)}{Cs_0}}{T}_j
}{
\This{(j+k)}{Cs_1}\quad \textit{with } \from{\This{n}{Cs_0}}{T}=\This{k}{Cs_1}
}
\defy{
\from{\This{n}{Cs}}{T}_j
}{
\from{\This{n}{Cs}}{T} \quad \textit{with }n<j
}
\defy{
p\op{minimize}{T}
}{
T'....
}
%This(j+n).Cs0[from P]j=This(j+k).Cs1
%  with Thisn.Cs0[from P]=Thisk.Cs1
%Thisn.Cs[from P]j=Thisn.Cs with n<j
%All cases for other expressions/terms propagate to submembers, including
%comments, since they can have annotated Ps
\end{defye}

\noindent Finally, we we combine those to notation for the
most common task of getting the value of a literal, in a way that can be understand from the current location: $p[t]$ and $p[T]$:
\begin{defye}%
  \defy{(DL_1\ldots DL_n; \_, t\eq{LV},\_)[t]}{\from{LV}{\This{n}}}
	\defy{p[T]}{p\op{minimize}{\from{p(T)}{T}}}%
\end{defye}
\\${}_{}$\\
%A declaration $D$ is just an $id = E$, representing that $id$ is declared to be the value of $E$, we also have $CD, CV, DL$, and $DV$ that constrain the forms of the LHS and RHS of the declaration.

%A literal $L$ has 4 components, an optional interface keyword, a list of implemented interfaces, a list of members, and an optional constructor. For simplicity, interfaces can only contain abstract-methods ($amt$) as members, and cannot have  constructors. A member $M$, is either an (potentially abstract) methood $mt$ or a nested class declaration $(CD)$. A member value $MV$, is a member that has been fully compiled. An $mid$ is an identifier, identifying a member.
%Constructors, $K$, contain a $Txs$ indicating the type and names of fields. An $e$ is normal fetherweight-java style expression, it has variables $x$, method calls $e.m(es)$, field accesses $e.x$ and object creation $new es$.

%$CtxV$ is the evaluation context for class-expressions $E$, and $ctxv$ is the usuall one for $e$’s.
\begin{comment}
Define operations on p
--------------------------------------
p.evilPush(L) = (C = L, p)
	for fresh C

p.push(id) = (id = L, p)
    p = (id' = {_;_, id = L, _ ;_}, _; Ds)

(id = L, p).pop() = p
(id = L, p).top() = L

Define equivy ops...
------------------------------
empty =p empty
P, Ps =p P', Ps' iff:
	p.minimize(P) = p.minimize(P')
	Ps =p Ps'

Pz subseteq_p Pz' iff:
	p.minimize(Pz) subseteq p.minimize(Pz')

p.minimize(empty) = empty
p.minimize(P, Pz) = p.minimize(P), p.minimize(Pz)

p.minimize(Thisn+1.idn.Cs) = p.minimize(Thisn.Cs):
  p = id0 = L0, ..., idn = Ln, _; Ds
  p(Thisn.Cs) = L
  // TODO: Check that Ln is an L instead?

otherwise p.minimize(P) = P

define dom(Mz) = Midz
===========================================
dom(empty) = empty
dom(C = E, Mz) = C, dom(Mz)
dom(T m(Txs), Mz) = m, dom(Mz)
\end{comment}

\section{Flattening}

Aside from the redirect operation itself, compilation/flattening is the most interesting part,
it is defined by reduction arrow $\Ds \Rightarrow \Ds'$, where eventually $\Ds'$ is going to reach form $\DVs$  and $p; \id \vdash E \Rightarrow E’$, where eventually $E'$ is going to reach form $LV$. The $\id$ represents the identifier of the type/trait that we are currently compiling, it is needed since it will be the name of $This0$, and we use that fact that that is equal to $This1.id$ to compare types for equality.
Rule \textsc{(Top)}  selects the leftmost $\id\eq{E}$
where $E$ is not of form $LV$ and $\DVz$: a 
well typed subset of the preceeding declarations. 
$E$ is flattened in the contex of such $DVz$, thus
by rule \textsc{(Trait)} $DVz$ must contain all the trait names used in $E$.

The \textsc{(CtxV)} rule is the standard context, the \textsc{(L-enter)} rule propegates compilation inside of nested-classes, \textsc{(trait)} merely evaluates a trait reference to it’s defined body,
finally \textsc{(sum)} and \textsc{(redirect)} perform our two meta-operations by propagating to 
corresponding auxiliary definitions.
%For simplicity rule \textsc{(sum)} is given in a highly non computational form,
%where non deterministically we select the result $LV_3$ and we use it in $p'$ and we also
%require it to be the result of 
$LV_1 \Q@<+@_{p'} LV_2 = LV_3$.
%This rule uses $p'$ only to check for errors.
Rule \textsc{(sum)} just delegate the work on the auxiliar notation defined on its right.
%On its right, we define the used auxiliary notation,
%showing how to sum literals and members.
As usual in definitions of sum operators,
the implemented interfaces is the union of the interfaces of $L_1$ and $L_2$, the members with the same domain are recursivelly composed while the members with disjoint domains are directly included.
Since method and nested class identifiers must be unique in a well formed $L$ and $M_1 \Q@<+@ M_2$  being defined only if the identifier is the same,
our definition forces $\dom{Mz}=\dom{Mz'}$ and
$\dom{Mz_1}$ disjoint $\dom{Mz_2}$.
For simplicity here 
 we require at most one class to have a state; if both have no state, the result will have no state, otherwise the result will have the only present state (
the set $\{empty,K?\}$ mathematically express this requirement in a compact way);
we also allow summing
only interfaces with interfaces and final classes with final classes. When two interfaces are composed both sides must define the same methods.
This is because other nested classes inside $L_1$ may be implementing such interface, and adding methods to such interface would require those classes to somehow add an implementation for those methods too.
In literature there are expressive ways to soundly handle merging different state, composing interfaces with final classes and
adding methods to interfaces, but they are out of scope in this work.

Member composition $M_1 \Q@<+@ M_2$ uses
the implementation from the right hand side, if available,
otherwise if the right hand side is abstract, the body is took from the left side.
Composing nested classes, not how they can not be \Q@private@; it is possible to sum two literals only if their private nested classes have different private names. This constraint can always be obtained by alpha-renaming them.
\begin{figure}
  \caption{Flattening}
\noindent$\begin{array}{l}
\textbf{Def:}\ Ds\Rightarrow Ds' \text{ and } p;id\vdash E\Rightarrow E',  \text{where   
\begin{bnf}
\production{%
\ctx{V}}{\hole \mmid{}  \summ{\ctx{V}}{E} %
                \mmid{}  \summ{LV}{\ctx{V}} \mmid{} \red{\ctx{V}}{Cs}{T}}  {}
\end{bnf}}\\

%\inferrule[(top)]{
%	a \xrightarrow[b]{} c\quad
%	\forall i<3 a\vdash b:\text{OK}\\\\
%	\forall i<3 a\vdash b:\text{OK}
%}{
%	1+2
%	\rightarrow
%	3
%}\begin{array}{l}
%a\\b\\c
%\end{array}
%\\
\inferrule[(Top)]{
\DVz \subseteq \DVs\\\\
\DVz \vdash \textbf{Ok}\\\\
\Empty; \DVz; id \vdash E \Rightarrow E'
}{
\DVs\ \id \Q@=@ E \Ds \Rightarrow \DVs\ \id \Q@=@ E' \Ds
}\quad

\quad\quad
\inferrule[(L-enter)]{
p\op{push}{id \Q{=} L[C = E]}; C \vdash E \Rightarrow E'
}{
p; \id \vdash L[C = E] \Rightarrow L[C = E']
}

\quad\quad
\inferrule[(trait)]{
}{
p; \id \vdash t \Rightarrow p[t]
}

\\[5ex]
\inferrule[(sum)]{
LV_1 \Q@<+@ LV_2 = LV_3%\\\\
%C' \textit{fresh} \\\\
%p'=p\op{push}{C'\Q@=@ LV_3}
}{
p; \id \vdash LV_1 \Q@<+@ LV_2 \Rightarrow LV_3
}
\quad
\begin{array}{l}
\text{\textbf{Def:}}\ L_1 \Q@<+@ L_2 = L_3 \\
\quad  L_1=\lib{Tz_1}{Mz,Mz_1}{K?_1}\\
\quad  L_2=\lib{Tz_2}{Mz',Mz_2}{K?_2}\\
\quad  L_3=\lib{Tz_1 \cup Tz_2}{Mz \Q@<+@ Mz',Mz_1,Mz_2}{K?}\\
\quad  \{\Empty, K?_1, K?_2\} = \{\Empty, K?\}\\
\quad  \text{if}\ \Q@interface@? = \Q@interface@\  \text{then}\ \mdom{L_1}= \mdom{L_2}\\
\text{\textbf{Def:}}\ T m\rp{Txs} e? \Q@<+@ T m\rp{Txs} e = T m\rp{Txs} e\\
\text{\textbf{Def:}}\ T m\rp{Txs} e? \Q@<+@ T m\rp{Txs} = T m\rp{Txs} e?\\
\text{\textbf{Def:}}\ (C \Q@=@ L) \Q@<+@
 (C \Q@=@ L') = C \Q@=@\ L \Q@<+@ L,% \text{with } p'=p\op{push}{C\Q@=@p(\This{0}{C}}
\end{array}


\\
\inferrule[(redirect)]{
  p' = p\op{push}{C\Q@=@ L}\\\\
  Csz = p'\op{redirectSet}{R}\\\\
  p'\op{redirectable}{Csz}\\\\
  R' =p'\op{bestRedirection}{R[\This{n}=\This{n+1}]}
}{
p; \id \vdash LV \Q@<@R\Q@>@ \Rightarrow   R'(L\op{remove}{Csz}) 
}


\end{array}$
\end{figure}


We have two-top level reduction rules defining our language, of the form $Ds e ––> Ds’ e$ which simply reduces the source-code.
The first rule $(compile)$ ‘compiles’ each top-level declaration (using a well-typed subset of allready compiled top-level declarations), this reduces the defining expresion.
The second rule, $(main)$ is executed once all the top-level declarations have compiled (i.e. are now fully evaluated class literals), it typechecks the top-level declarations and the main expression, and then procedes to reduce it.
In principle only one-typechecking is needed, but we repeat it to avoid declaring more rules.
\begin{verbatim}
Define Ds e --> Ds' e'
================================================================
DVs' |- Ok
empty; DVs'; id | E --> E'
(compile)---------------------------------------- DVs' subsetof DVs
DVs id = E Ds e --> DVs id = E' Ds e

DVs |- Ok
DVs |- e : T
DVs |- e --> e'
(main)---------------------------------- for some type T
DVs e --> DVs e'
\end{verbatim}

gff
\begin{defye}%
	\defy{L[C\eq{E'}]}{\lib{Tz}{\s{MV}\ C\eq{E'}\ \s{M}}{K?}}
	\defyc{\text{where } L = \lib{Tz}{\s{MV}\ C\eq{\_}\ \s{M}}{K?}}
	\defy{\s{T} \in p}{\forall T \in \s{T} \bullet p(T) \text{ is defined}}% WHERE WE USE IT?
\end{defye}


%We will also use $p[T.m]$ to extra

\begin{bnf}
\production{%
\ctx{V}}{\hole \mmid{}  \summ{\ctx{V}}{E} %
                \mmid{}  \summ{LV}{\ctx{V}} \mmid{} \red{\ctx{V}}{Cs}{T}}  {context of library-evaluation}\\\production{%
%LV}     {\libi{Tz}{amtz}{}\ \ \mmid{}\ \ \libc{Tz}{MVs}{K$?$}}       {literal value}\\\production{%
%MV}     {C\eq{}LV \mmid{} mt}                                                    {}\\\production{%
\ctx{v}}{\hole \mmid{}  \ctx{v}\Q{.}m\rp{es} \mmid{}  v\Q{.}m\rp{vs \ctx{v} es} %
	\mmid{} T\Q{.}m\rp{vs \ctx{v} es}  }           {}%\\\production{%
%DL}     {id\eq{}L}                                                         {partially-evaluated-declaration}\\\production{%
%DV}     {id\eq{}LV}                                                       {evaluated-declaration}\\\production{%
%Mid}    {C \mmid{} m}                                                      {member-id}\\\production{%
%p}      {DLs\Q{;} DVs}                                                     {program}
\end{bnf}



\section{Type System}

The type system is split into two parts: type checking programs and class literals, and the typechecking of expressions. The latter part is mostly convential, it involves typing judgments of the form $p; Txs \vdash e : T$, with the usual program $p$ and variable environement $Txs$ (often called $\Gamma$ in the literature). rule ($Ds ok$) type checks a sequence of top-level declarations by simply push each declaration onto a program and typecheck the resulting program.
Rule $p ok$ typechecks a program by check the topmost class literal: we type check each of it’s members (including all nested classes), check that it properly implements each interface it claims to, does something weird, and finanly check check that it’s constructor only referenced existing types,

\begin{verbatim}


Define p |- Ok
===========================================================

D1; Ds |- Ok ... Dn; Ds|- Ok
(Ds ok) ------------------------------ Ds = D1 ... Dn
Ds |- Ok

p |- M1 : Ok .... p |- Mn : Ok
p |- P1 : Implemented .... p |- Pn : Implemented
p |- implements(Pz; Ms) /*WTF?*/                   if K? = K: p.exists(K.Txs.Ts)
(p ok) ------------------------------------------- p.top() = interface? {P1...Pn; M1, ..., Mn; K?}
p |- Ok

p.minimize(Pz) subseteq p.minimize(p.top().Pz)
amt1 _ in p.top().Ms ... amtn _ in p.top().Ms
(P implemented) ----------------------------------------------- p[P] = interface {Pz; amt1 ... amtn;}
p |- P : Implemented

(amt-ok) ------------------- p.exists(T, Txs.Ts)
p |- T m(Tcs) : Ok

p; This0 this, Txs |- e : T
(mt-ok) ------------------------------ p.exists(T, Txs.Ts)
p |- T m(Tcs) e : Ok

C = L, p |- Ok
(cd-Ok) -------------------
p |- C = L : OK

\end{verbatim}

Rule $(P implemented)$ checks that an interface is properly implemented by the program-top, we simply check that it declares that it implements every one of the interfaces super-interfaces and methods.
Rules $(amt-ok)$ and $(mt-ok)$ are straightforward, they both check that types mensioned in the method signature exist, and ofcourse for the latter case, that the body respects this signature.

To typecheck a nested class declaration, we simply push it onto the program and typecheck the top-of the program as before.


The expression typesystem is mostly straightforward and similar to feartherwieght Java, notable we we use $p[T]$ to look up information about types, as it properly ‘from’s paths, and use a classes constructor definitions to determine the types of fields.

\begin{verbatim}
Define p; Txs |- e : T
=====================================
(var)
----------------------- T x in Txs
p;  Txs |- x : T

(call)
p; Txs |- e0 : T0
...
p; Txs |- en : Tn
-----------------------------------  T' m(T1 x1 ... Tn xn) _ in p[T0].Ms
p; Txs |- e0.m(e1 ... en) : T'

(field)
p; Txs |- e : T
---------------------------------------  p[T].K = constructor(_ T' x _)
p; Txs |- e.x : T'


(new)
p; Txs |- e1 : T1 ... p; Txs |- en : Tn
------------------------------------------- p[T].K = constructor(T1 x1 ... Tn xn)
p; Txs |- new T(e1 ... en)


(sub)
p; Txs |- e : T
-----------------------------------  T' in p[T].Pz
p; Txs |- e : T'


(equiv)
p; Txs |- e : T
-----------------------------------  T =p T'
p; Txs |- e : T'
\end{verbatim}

\section{Graph example}
We now consider an example where Redirect simplifies the code quite a lot:
We have a \Q@Node@ and \Q@Edge@ concepts for a graph.
The \Q@Node@ have a list of \Q@Edge@s.
A \Q@isConnected@ function takes a list of \Q@Node@s.
A \Q@getConnected@ function takes \Q@Node@ and return a set of \Q@Node@s.
\begin{lstlisting}
graphUtils={
  Edges:list<+{Node start() Node end()}
  Node:{Edges connections()}
  Nodes:set<Elem=Node>//note that we do not specify equals/hash
  static Bool isConnected(Nodes nodes)=
    if(nodes.size()=0) then true
    else getConnected(nodes.asList().head()).size()==nodes.size()
  static Nodes getConnected(Node node)=getConnected(node,Nodes.empty())
  static Nodes getConnected(Node node,Nodes collected)=
    if(collected.contains(node)) then collected
    else connectEdges(node.connections(),collected.add(node))
  static Nodes connectEdges(Edges e,Nodes collected)=
    if( e.isEmpty()) then collected
    else connectEdges(e.tail(),collected.add(e.head().end()))
  }
\end{lstlisting}

We have shown the full code instead of omitting implementations to show that
the code inside of an highly general code like the former is pretty conventional.
Just declare nested classes as if they was the concrete desired types. Note how we can easly create a new \@Nodes@ by doing \Q@Nodes.empty()@.

Here we show how to instantiate \Q@graphUtils@ to a graph representing cities connected by streets,
where the streets are annotated with their length, and \Q@Edges@ is a priority queue, to optimize
finding the shortest path between cities.

\begin{lstlisting}
Map:{
  Street:{City start,City end, Int size}
  City:{}
  Streets:priorityQueue<Elem=Street><+{    
    Int geq(Street e1,Street e2)=e1.size()-e2.size()}
  }<+{
  Streets:{}
  City:{Streets connections, Int index}//index identify the node
  Cities:set<Elem=City><+{
    Bool eq(City e1,City e2) e1.index==e2.index
    Int hash(City e) e.index
    }
  Cities cities
  //more methods
  }
MapUtils=graphUtils<Nodes=Map.Cities>
//infers Nodes.List, Node, Edges, Edge
\end{lstlisting}

In Appending 2 we will show our best attempt to encode this graph example in Java, Rust and Scala.
In short, we discovered...
