\section{Language grammar and well formedness}
We apply our ideas on a simplified object oriented language with nominal typing and (nested)
interfaces  and final classes.
Code reuse is obtained by trait composition, thus the source code would be
a sequence of top level declarations $D$ followed by a main expression;
a lower-case identifier $t$ is a trait name, while an upper case
identifier $C$ is a class name.
To simplify our terminology, instead of distinguishing between 
nested classes and nested interfaces, we will call \emph{nested class} any member of a code literal 
named by a class identifier $C$. Thus, the term \emph{class} may denote either an \emph{interface class} (interface for short) or a \emph{final class}.

\noindent
\begin{minipage}{0.65\textwidth}
\begin{bnf}
\hline
\production{%
e}      {x \mmid{} e\Q{.}m\rp{es} \mmid{}T\Q{.}m\rp{es}
\mmid{} e\Q{.}x \mmid{} \Q{new} T\rp{es}
}{expression}\\\production{%
L}      {\libi{Tz}{Ms}
%}{interface literal}
%\\\prodNextLine{%
\mmid{} \libc{Tz}{Mz}{K}
}        {code literal}\\\production{%
M}      {\Q{static}$?$ T m\rp{Txs} e$?$ \mmid{} \Q{private}$?$ C\eq{}E }                                                    {member}\\\production{%
K}      {\rp{Txz}$?$}                                          {state}\\\production{%
%CD}     {C\eq{}E}                                                          {class declaration}\\\production{%
%CV}     {C\eq{}LV}                                                         {evaluated class declaration}\\\production{%
%source}      {Ds e}                                                             {source code}\\\production{%
E}      {L \mmid{} t \mmid{} \summ{$E_1$}{$E_2$} \mmid{} E\Q@<@R\Q@>@}           {Code Expr.}%
%amt}    {T m\rp{Txs}}                                                      {abstract method}\\\production{%
%mt}     {\Q{static}$?$ T m\rp{Txs} e$?$}                                                 {method}\\\production{%
\\\production{%
R}      {$Cs_1 \eq{T_1}\ldots Cs_n\eq{T_n}$}                    {redirect map}\\\hline
\end{bnf}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{bnf}
\hline
\production{%
T}      {\Q{This}n\Q{.}Cs}                                                 {types}\\\production{%
Tx}     {T x}                                                              {parameter}\\\production{%
D}      {id\eq{}E}                                                         {declaration}\\\production{%
id}     {C \mmid{} t}                                                      {class/trait id}\\\production{%
v}      {\Q{new} T\rp{vs}}                                                 {value}\\\production{%
LV}      {\ldots}{}                                                
\\\hline
\end{bnf}
\end{minipage}


In the context of nested classes, types are paths. Syntactically,
we represent them as relative paths of form 
$\This{n}{Cs}$, where the number $n$ identify the root of our path:
\Q@This@/\This0\ is the current class, \This1\ is the enclosing class, \This2\ is the enclosing enclosing class and so on. $\This{n}{Cs}$
refers to the class obtained by navigating throughout  $Cs$ starting from $\This{n}$.
By using a larger then needed $n$, there could be multiple different types referring to the same class.
We require all types to be in the form where the smallest possible $n$ is used.

Code literals $L$
serve the role of class/trait bodies; they contain the set of implemented interfaces
$Tz$, the set of members $Mz$ and their (optional) state.
In the concrete syntax we will use \Q@implements@ in front of a non empty list of implemented interfaces
and we will omit parenthesis around a non empty set of fields.
To simplify our formalism, we delegate some sanity checks to well formedness: all the fields in the state $K$ have different names;
no two methods or nested classes with the same name ($m$ or $C$) are declared in a code literal,
and no nested class is named $\This{n}$ for any number $n$;
in any method headers, all parameters have different names, and no parameter is named \Q@this@.

A class member $M$ can be a (private) nested class or a (static) method.
Abstract methods are just methods without a body. 
Well formed interface methods can only be abstract and non-static.
To facilitate code reuse, classes can have (static) abstract methods; code composition is expected to 
provide an implementation for those or, as we will see, redirect away the whole class.
We could easily support private methods too, but to simplify our formalism we consider private only for nested classes. In a well formed code literal, in all types of form $\This{n}{Cs}{C}{Cs'}$,
if $C$ denotes a private nested class, then $Cs$ is empty.

Expressions are used as body of (static) methods and for the main expression.
They are variables $x$ (including \Q@this@)
and conventional (static) method calls.
Field access and \Q@new@ expressions are included but with restricted usage:
well formed field accesses are of form \Q@this.@$x$ in method bodies and
$v$\Q@.@$x$  in the main expression, while 
well formed \Q@new@ expressions have to be of form \Q@new This0(@$xs$\Q@)@ in method bodies
and of form $v$ in the main expression.
Those restrictions greatly simply reasoning about code reuse, since they require different classes to
only communicate by calling (static) methods. Supporting unrestricted fields and constructors would make the formalism much more involved without adding much of a conceptual underpinning.
Values are of form \textit{\Q{new} T\rp{vs}}.

For brevity, in the concrete syntax we assume a syntactic sugar declaring
a static \Q@of@ method (that serve as a factory) and all fields getters; 
thus the order of the fields would induce the order of the factory arguments.
In the core calculus we just assume such methods to be explicitly declared.

Finally, we examine the shape of a nested class: \textit{\Q{private}$?$ C\eq{}E}.
The right hand side is not just a code literal but a code composition expression $E$.
In trait composition, the code expression will be reduced/flattened to a code literal $L$
during compilation.
Code expressions denote an algebra of code composition, starting from code literal $L$
and trait names $t$, referring to a literal declared before by \textit{t\eq{}E}.
We consider two operators: conventional preferential sum 
 \textit{\summ{$E_1$}{$E_2$}} and our novel redirect\textit{\red{E}{Cs}{T}}.

\subsection{Compilation process/flattening}
The compilation process consists in flattening all the $E$ into $L$,
starting from the innermost leftmost $E$. This means that sum and redirect work on $LV$s:
a kind of $L$, where all the nested classes are of form \textit{\Q@private@? C\eq{}LV}.
The execution happens after compilation and consist in the conventional execution of the main expression $e$
in the context of the fully reduced declarations, where all trait composition has
been flatted away.
Thus, execution is very simple and standard and behaves like a variation of FJ[] with interfaces
instead of inheritance, and where nested classes are just a way to hierarchically organize code names.
On the other side, code composition in this setting is very interesting and powerful, where
nested classes are much more than name organization: they support in a simple and intuitive way
expressive code reuse patterns.
To flatten an $E$ we need to understand the behaviour of the two operators, and how to 
load the code of a trait: since it was written in another place, the syntactic representation of
the types need to be updated.
For each of those points we will first provide some informal explanation and then we will proceed formalizing
the precise behaviour.

\subsubsection{Redirect}

Redirect %\textit{\red{LV}{$Cs_1:T_1\ldots Cs_n:T_n$}{}}
takes a library literal and produce a modified version of it where some nested classes has been removed
and all the types referencing such nested classes are now referring to an external type. It is easy to use this feature to encode a generic list:

%final class LinkedList<T> {
%  boolean isEmpty(){ return impl == Impl.empty; }
%  T head() { return impl.asCons().elem; }
%  LinkedList tail(){ return impl.asCons().tail; }
%  LinkedList cons(T e){return new LinkedList<T>(){{
%    impl=new Cons(){{elem=e; tail=impl}};}};}
%  private final Supplier<Cons> impl = empty;
%  static final Supplier<Cons> empty = ()->{throw ..;};
%  private static class Cons implements Supplier<Cons>{
%    T elem; Impl tail; Cons get(){return this;}}
%  } 
\begin{lstlisting}
list={
  Elem={}
  static This0 empty()= new This0(Empty.of())
  boolean isEmpty()= this.impl().isEmpty()
  Elem head()= this.impl.asCons().tail()
  This0 tail()=this.impl.asCons().tail()
  This0 cons(Elem e)=new This0(Cons.of(e, this.impl)
  private Impl={interface   Bool isEmpty()  Cons asCons()}
  private Empty={implements This1
    Bool isEmpty()=true  Cons asCons()=../*error*/
    ()}//() means no fields
  private Cons={implements This1
    Bool isEmpty()=false  Cons asCons()=this
    Elem elem Impl tail }
  Impl impl
  }
IntList=list<Elem=Int>
...
IntList.Empty.of().push(3).top()==4 //example usage
\end{lstlisting}
This would flatten into
\begin{lstlisting}
list={/*as before*/
//IntList=list<Elem=Int>
IntList={
  //Elem={} no more nested class Elem
  static This0 empty()= new This0(Empty.of())
  boolean isEmpty()= this.impl().isEmpty()
  Int head()= this.impl.asCons().tail()
  This0 tail()=this.impl.asCons().tail()
  This0 cons(Int e)=new This0(Cons.of(e, this.impl)
  private Impl={interface   Bool isEmpty()  Cons asCons()}
  private Empty={/*as before*/}
  private Cons={implements This1
    Bool isEmpty()=false  Cons asCons()=this
    Int elem Impl tail }
  Impl impl
  }//everywhere there was "Elem", now there is "Int"
\end{lstlisting}

Redirect can be propagated in the same way generics parameters are propagate:
For example, in Java one could write code as below,
\begin{lstlisting}
class ShapeGroup<T extends Shape>{
  List<T> shapes;
  ..}
//alternative implementation
class ShapeGroup<T extends Shape,L extends List<T>>{
  L shapes;
  ..}
\end{lstlisting}
to denote a class containing a list of a certain kind of \Q@Shape@s.
In our approach, one could write the equivalent
\begin{lstlisting}
shapeGroup={
  MyShape={implements Shape}
  List=list<Elem=MyShape>
  List shapes
  ..}
\end{lstlisting}
With redirect, \Q@shapeGroup@ follow both roles of the two Java examples;
indeed there are two reasonable ways to reuse this code

\Q@Triangolation=shapeGroup<MyShape=Triangle>@,
if we have a \Q@Triangle@ class and we would like the concrete list type
used inside to be local to the \Q@Triangolation@,\\*
or 
\Q@Triangolation=shapeGroup<List=Triangles>@,
if we have a preferred implementation for the list of triangles that is going to be used by our
\Q@Triangolation@.
Those two versions would flatten as follow:
\begin{lstlisting}
//Triangolation=shapeGroup<MyShape=Triangle>
Triangolation={
  List=/*list with Triangle instead of Elem*/
  List shapes
  ..}

//Triangolation=shapeGroup<List=Triangles>
//exapands to shapeGroup<List=Triangles,MyShape=Triangle>
Triangolation={
  Triangles shapes
  ..}
\end{lstlisting}
As you can see, with redirect we do not decide a priori what is generic and what is not.

Redirect can not always succeed. For example, if we was to attempt
\Q@shapeGroup<List=Int>@ the flattening process would fail with an error similar to a 
invalid generic instantiation.

\subsubsection{Preferential sum; sum and redirect working together}
The sum of two traits is conceptually a trait with the sum of the traits members, and the union of the implemented interfaces.
If the two traits both define a method with the same name, some resolution strategy is applied.
In the symmetric sum[] the two methods need to have the same signature and at least one of them need to be abstract.
With preferential sum (sometimes called override), if they are both implemented, the right implementation is chosen and the left one is discarded.
Since in our model we have nested classes, nested classes with the same name will be recursively composed.

We chose preferential sum since is simpler to use in short code examples.~\footnote{symmetric sum is often presented in conjunction with a restrict operator that makes some methods abstract.}
Since the focus of the paper is the novel redirect operator, instead of the well known sum, we will handle summing state and interfaces in the simplest possible way:
a class with state can only be summed with a class without state, and an interface can only be summed with another interface with identical methods signatures.

In literature it has been shown how trait composition with (recursively composed) nested classes can 
elegantly handle the expression problem and a range of similar design challenges.
Here we will show some examples where sum and redirect cooperate to produce interesting code reuse patterns:

\begin{lstlisting}
listComp=list<+{
  Elem:{ Int geq(This e)}//-1/0/1 for smaller, equals, greater
  static Elem max2(Elem e1, Elem e2)=if e1.geq(e2)>0 then e1, else e2
  Elem max(Elem candidate)=
    if This.isEmpty() then candidate
    else this.tail().max(This.max2(this.head(),candidate))
  Elem min(Elem candidate)=...
  This0 sort()=...
  }
\end{lstlisting}
As you can see, we can \emph{extends} our generic type while refining our generic argument:
\Q@Elem@ of \Q@listComp@ now needs a \Q@geq@ method.

While this is also possible with conventional inheritance and F-Bound polymorphism, we think this solution is logically simpler then the equivalent Java
\begin{lstlisting}
class ListComp<Elem extends Comparable<Elem>> extends LinkedList<Elem>{
  ../*body as before*/
  }
\end{lstlisting}

Another interesting way to use sum is to modularize behaviour delegation: consider the following
(not efficient for the sake of compactness) implementation of \Q@set@, where the way to compare elements is not fixed:
\begin{lstlisting}
set:{
  Elem:{}
  List=list<Elem=Elem>
  static This0 empty()= new This0(List.empty())
  Bool contains(Elem e)=../*uses eq and hash*/
  Int size()=..
  This add(Elem e)=...
  This remove(Elem e)=...
  Bool eq(Elem e1,Elem e2)//abstract
  Int hash(Elem e)//abstract
  List asList //to allow iteration
  }
eqElem={
  Elem={ Bool equals(Elem e)/*abstract*/}
  Bool eq(Elem e1,Elem e2)=e1.equals(e2)
  }
hashElem={
  Elem={ Int hash(Elem e)/*abstract*/}
  Int hash(Elem e)=e.hash()
  }
Strings=(set<+eqElem<+eqHash)<Elem=String>
LongStrings=(set<+eqElem)<Elem=String> <+{
  Int hash(String e)=e.size()
  }//for very long strings, size is a faster hash
\end{lstlisting}
Note how 
\Q@(set<+eqElem<+eqHash)<Elem=String>@
is equivalent to\\*
 \Q@set<Elem=String> <+eqElem<Elem=String> <+eqHash<Elem=String>@.

Consider the signature \Q@Bool equals(Elem e)@.
This is different from the common signature \Q@Bool equals(Object e)@. What is the best
signature for \Q@equals@ is an open research question, where most approaches advise either the
first or the second one. Our \Q@eqElem@, as written, can support both:
\Q@Strings@ would be correctly define both if \Q@String.equals@ signature
has a \Q@String@ or an \Q@Object@ parameter.EXPAND on method subtyping.


\subsection{Moving traits around in the program}
It is not trivial to formalize the way types like \Q@This1.A.B@ %$\This{3}{\Q@A@}{\Q@B@}$
have to be adapted so that when code is moved around in different depths of nesting the 
refereed classes stay the same.
This is needed during flattening, when a trait $t$ is reused, but also during reduction, when a method body is inlined in the main expression, and during typing, where a method body is typed depending on the signature of other methods in the system.

To this aim we define a concept of program 
$\textit{p}\Coloneqq\textit{Ds\Q{;} DVz}$
where 
$\textit{DV}\Coloneqq\textit{id\eq{LV}}$; as a representation of 
the code as seen from a certain point inside of the source code. It is the most interesting form of the grammar,
used for virtually all reduction and typing rules. On the left of the `$;$' is a stack representing which (nested) declaration is currently being processed,
 the bottom of the stack (rightmost) \textit{D} represents the top level declaration of the source-program that is currently being processed, while the other elements of the stack are nested classes nested inside of each other.
  The right of the `$;$' represents the top-level declarations that have already been compiled, this is necessary to look up top-level classes and traits.
Summarizing, each of the $\textit{D}_0\ldots\textit{D}_n$
represents the outer nested level $0..n$, while
the \textit{DVs} component represent the already flattened portion of the program top level, that is 
the outer nested level $n+1$
%\begin{bnf}
%\production{%
%p}      {Ds\Q{;} DVs}                                                     {program}\\\production{%
%DL}     {id\eq{}L}                                                         {partially-evaluated-declaration}\\\production{%
%DV}     {id\eq{}LV}                                                       {evaluated-declaration}%\\\production{%
%Mid}    {C \mmid{} m}                                                      {member-id}%\\\production{%
%\end{bnf}
Thus, for example in the program
\begin{lstlisting}
A={()}
t={ B={()}   This1.A m(This0.B b)}
C={D={E=t}}
H=t<B=A>
\end{lstlisting}
the flattened body of \Q@C.D.E@ will be 
\Q@{ B={()}   This3.A m(This0.B b)}@, where the path
\Q@This1.A@ is now \Q@This3.A@ while the path \Q@This0.B@ stays the same: types defined internally will
stay untouched.
The program $p$ in the observation point \Q@E=t@ is
\begin{lstlisting}
A={()}
t={ B={()}   This1.A m(This0.B b)}
C={D={E=t}};
C={D={E=t}},//this means, we entered in C
D={E=t}//this means, we entered in D
\end{lstlisting}

%We will use $p$ as a function, so we will write
%.....p(T) and p(T.m)
%we define now From,
%....

In order to fetch the code literals corresponding to $t$,
we define notation $p[t]$\\*
(=\Q@{ B={()}   This3.A m(This0.B b)}@).
Such notation transforms the types so that they keep referring to the same nested classes.
We also rely on the notation $p[T]$, to extract just methods and the list of implemented interfaces, in a form were they are useful for direct comparison with 
$T$.
for example, if the program contains \Q@{B={}  This0 m(This0.B x)}@
in position 
\Q@This2.A@, $p[$\Q@This2.A@$]$ would be 
\Q@{This2.A m(This2.A.B x)}@.
%We also use notation $L[Cs=E]$ to update the code expression in $Cs$ to $E$,
%and $p\op{min}{T}=T'$ to minimize types to the required form when the $n$
%is as small as possible.



We now present formal definition for those operations.
We will use members $Mz$ as a function containing both method names $m$
and class names $C$ in its domain; thus we will assume
notation $\dom{Mz}$, $Mz(m)$, $Mz(C)$ with the usual meaning.
Under here, we define useful auxiliary notations to
access literals $L$ with functional notation with the intent of accessing their members. We define notations $L[Cs=E]=L'$ and $Mz[C=E]=Mz'$ serving the role of function update.
We use those notations to define $p(T)=LV$ accessing a program $p$ as function. We also define operations on programs: $p\op{push}{D}=p'$, allowing to work with programs as if they was stacks, and
$p\op{min}{T}=T'$, denoting the shortest type $T'$ referring to the same 
nested class of $T$.
We define $\from{T}{T',j}$ and $\from{L}{T,j}$; we omit all the trivial propagation cases of form $\from{M}{T,j}$, $\from{K}{T,j}$ and $\from{e}{T,j}$.
%Finally, we we combine those to notation for the
%most common task of getting the value of a literal, in a way that can be understand from the current location: $p[t]$ and $p[T]$:


\noindent
\begin{minipage}{0.48\textwidth}
\noindent\!\!\!$\begin{array}{l}
\hline
(\DLs\Q@;@ \DVs)\op{push}{id\eq{L}} =
\id\eq{L},\DLs\Q@;@ \DVs\\
\hline
(; \_, C\eq{L}, \_)(\This{0}{C}{Cs})=\mathit{L(Cs)}\\
p\op{push}{\_\eq{L}}(\This{0}{Cs})=L(\mathit{Cs})\\
p\op{push}{\_}(\This{n+1}{Cs})=p(\This{n}{Cs})\\
\hline
\fop{members}{\lib{\_}{Mz}{\_}}=Mz\\\hline
L(m)=\fop{members}{L}(m)\\\hline
L(C)=\fop{members}{L}(C)\\\hline
\dom{L}=\dom{\fop{members}{L}}\\\hline
\mdom{L}=\{m \in \dom{L}\}\\
\end{array}$
\end{minipage}%
\begin{minipage}{0.5\textwidth}
$\begin{array}{ll}
 &\hline
(Mz,\Q@private@? C\eq{\_})[C=E]=Mz,\Q@private@? C\eq{E}\\\hline
 &LV({\emptyset})=LV\\
 &L(C\Q@.@Cs)=L(C)(Cs)\\
 &\hline
L[\Empty=E] = E\\
 &\lib{Tz}{Mz}{K?}[C\Q@.@Cs=E]=\\ &\quad
\lib{Tz}{Mz[C=Mz(C)[Cs=E]]}{K?}
\\\hline
 &p\op{min}{\This{n+1}{\id_n}{Cs}} = p\op{min}{\This{n}{Cs}}\\ &\quad
  \text{where }p = \id_0 \eq{L_0}\ldots\id_n\eq{L_n} \_\Q@;@ Ds\\
 &\text{otherwise } p\op{min}{T} = T\\
\end{array}$
\end{minipage}

\noindent\!\!\!\!
$\begin{array}{l}
\hline
\from{\This{n}{Cs}}{T,j}
=
\This{n}{Cs} \quad \textit{with }n<j
\\
\from{\This{n+j}{Cs}}{\This{m}{C_1\ldots C_k},j}
=
\This{m+j}{C_1\ldots C_{k-n}} \quad \textit{with }n\leq k
\\
\from{\This{n+j}{Cs}}{\This{m}{C_1\ldots C_k},j}
=
\This{m+j+n-k}{C_1\ldots C_{k-n}{Cs}} \quad \textit{with }n> k
\\
\from{
\libc{\Q@interface@? Tz}{Mz}{K}
}{T,j-1}
=
\libc{\Q@interface@? \from{Tz}{T,j}}{\from{Mz}{T,j}}{\from{K}{T,j}}
\\\hline
(DL_1\ldots DL_n; \_, t\eq{LV})[t]
=p\op{min}{\from{LV}{\This{n},0}}\\\hline
p[T]=p\op{min}{
\lib{\from{Tz}{T,0}}{\from{Mz}{T,0}}{}
}
\quad\text{where }p(T)=\lib{Tz}{Mz}{K?}
\\\hline
\end{array}$


%For space reasons, those notations are defined in the appendix.
The type system and the reduction of the main program are in appendix. They are very straight forward: thanks to flattening, they are a simple nominal type system and reduction over a FJ-like language, with no generics or special method dispatch rules.


%A declaration $D$ is just an $id = E$, representing that $id$ is declared to be the value of $E$, we also have $CD, CV, DL$, and $DV$ that constrain the forms of the LHS and RHS of the declaration.

%A literal $L$ has 4 components, an optional interface keyword, a list of implemented interfaces, a list of members, and an optional constructor. For simplicity, interfaces can only contain abstract-methods ($amt$) as members, and cannot have  constructors. A member $M$, is either an (potentially abstract) methood $mt$ or a nested class declaration $(CD)$. A member value $MV$, is a member that has been fully compiled. An $mid$ is an identifier, identifying a member.
%Constructors, $K$, contain a $Txs$ indicating the type and names of fields. An $e$ is normal fetherweight-java style expression, it has variables $x$, method calls $e.m(es)$, field accesses $e.x$ and object creation $new es$.

%$CtxV$ is the evaluation context for class-expressions $E$, and $ctxv$ is the usuall one for $e$’s.
\begin{comment}
Define operations on p
--------------------------------------
p.evilPush(L) = (C = L, p)
	for fresh C

p.push(id) = (id = L, p)
    p = (id' = {_;_, id = L, _ ;_}, _; Ds)

(id = L, p).pop() = p
(id = L, p).top() = L

Define equivy ops...
------------------------------
empty =p empty
P, Ps =p P', Ps' iff:
	p.minimize(P) = p.minimize(P')
	Ps =p Ps'

Pz subseteq_p Pz' iff:
	p.minimize(Pz) subseteq p.minimize(Pz')

p.minimize(empty) = empty
p.minimize(P, Pz) = p.minimize(P), p.minimize(Pz)

p.minimize(Thisn+1.idn.Cs) = p.minimize(Thisn.Cs):
  p = id0 = L0, ..., idn = Ln, _; Ds
  p(Thisn.Cs) = L
  // TODO: Check that Ln is an LV instead?

otherwise p.minimize(P) = P

define dom(Mz) = Midz
===========================================
dom(empty) = empty
dom(C = E, Mz) = C, dom(Mz)
dom(T m(Txs), Mz) = m, dom(Mz)
\end{comment}

\section{Flattening}

%Aside from the redirect operation itself, compilation/flattening is the most interesting part.
Flattening is defined by reduction arrow $\Ds \Rightarrow \Ds'$, where eventually $\Ds'$ is going to reach form $\DVs$  and $p; \id \vdash E \Rightarrow E’$, where eventually $E'$ is going to reach form $LV$. The $\id$ represents the identifier of the type/trait that we are currently compiling, it is needed since it will be the name of \This0, and we use to the fact that refers to the same nested class as $\This1{\id}$.
Rule \textsc{(Top)}  selects the leftmost $\id\eq{E}$
where $E$ is not of form $LV$ and $\DVz$: a 
well typed subset of the preceeding declarations. 
$E$ is flattened in the contex of such $\DVz$, thus
by rule \textsc{(Trait)} $\DVz$ must contain all the trait names used in $E$.
In the judgement $p; \id \vdash E \Rightarrow E’$
$\id$ is only used in order to grow the program $p$ in rule 
\textsc{(L-enter)}, and $p$ itself is only needed for 
\textsc{(redirect)}.
The \textsc{(CtxV)} rule is the standard context, the \textsc{(L-enter)} rule propegates compilation inside of nested-classes, \textsc{(trait)} merely evaluates a trait reference to it’s defined body,
finally \textsc{(sum)} and \textsc{(redirect)} perform our two meta-operations by propagating to 
corresponding auxiliary definitions. We will present those two rules in the two sections below.
Note how we require their input to be already in the \emph{minimized}
form, that is, all the $T$ uses the shortest way to refer to their corresponding nested class. 
This prevents the programmer from expressing some difficult cases. Consider for example using two different ways to refer to $A$, redirect $A$ and then adding it back:
\begin{lstlisting}
B=...
X={ A:{}    Void m(This1.X.A p1, This0.A p2)} <A=B> <+ {A:{}}
//should flattening redirect only p2 or also p1
X={ A:{}    Void m(??? p1, This1.B p2)}
\end{lstlisting}
The complete L42 language solves those issues, but here we present a simplified version.
%For simplicity rule \textsc{(sum)} is given in a highly non computational form,
%where non deterministically we select the result $LV_3$ and we use it in $p'$ and we also
%require it to be the result of 
%$LV_1 \Q@<+@_{p'} LV_2 = LV_3$.
%This rule uses $p'$ only to check for errors.
\subsection{Sum}
Rule \textsc{(sum)} just delegate the work on the auxiliary notation defined below:

\noindent$\begin{array}{l}
\hline
L_1 \Q@<+@ L_2 = \lib{Tz_1 \cup Tz_2}{Mz \Q@<+@ Mz',Mz_1,Mz_2}{K?} \\
\quad  L_1=\lib{Tz_1}{Mz,Mz_1}{K?_1},\quad
\quad  L_2=\lib{Tz_2}{Mz',Mz_2}{K?_2}\\
\quad  \{\Empty, K?_1, K?_2\} = \{\Empty, K?\}\\
\quad  \text{if}\ \Q@interface@? = \Q@interface@\  \text{then}\ \mdom{L_1}= \mdom{L_2}\\
\hline
T m\rp{Txs} e? \Q@<+@ T m\rp{Txs} e = T m\rp{Txs} e\\
T m\rp{Txs} e? \Q@<+@ T m\rp{Txs} = T m\rp{Txs} e?\\
(C \Q@=@ L) \Q@<+@
 (C \Q@=@ L') = C \Q@=@\ L \Q@<+@ L,% \text{with } p'=p\op{push}{C\Q@=@p(\This{0}{C}}
\\\hline
\end{array}$

%On its right, we define the used auxiliary notation,
%showing how to sum literals and members.
As usual in definitions of sum operators,
the implemented interfaces is the union of the interfaces of $L_1$ and $L_2$, the members with the same domain are recursivelly composed while the members with disjoint domains are directly included.
Since method and nested class identifiers must be unique in a well formed $L$ and $M_1 \Q@<+@ M_2$  being defined only if the identifier is the same,
our definition forces $\dom{Mz}=\dom{Mz'}$ and
$\dom{Mz_1}$ disjoint $\dom{Mz_2}$.
For simplicity here 
 we require at most one class to have a state; if both have no state, the result will have no state, otherwise the result will have the only present state (the set $\{empty,K?\}$ mathematically express this requirement in a compact way);
we also allow summing
only interfaces with interfaces and final classes with final classes. When two interfaces are composed both sides must define the same methods.
This is because other nested classes inside $L_1$ may be implementing such interface, and adding methods to such interface would require those classes to somehow add an implementation for those methods too.
In literature there are expressive ways to soundly handle merging different state, composing interfaces with final classes and
adding methods to interfaces, but they are out of scope in this work.

Member composition $M_1 \Q@<+@ M_2$ uses
the implementation from the right hand side, if available,
otherwise if the right hand side is abstract, the body is took from the left side.
Composing nested classes, note how they can not be \Q@private@; it is possible to sum two literals only if their private nested classes have different private names. This constraint can always be obtained by alpha-renaming them:
we assume a form of alpha-reaming for private nested classes, that will consistently rename all the 
paths of form $\This{n}{C}{Cs'}$, where 
$\This{n}{C}$ refer to such private nested class. The trivial definition of such alpha renaming is given in the appendix.


\subsection{Redirect}
Rule \textsc{(redirect)} is the centre of our interest for this work. As for sum we check that the $LV$ is in minimized form.
Moreover, to have a single data structure $p'$ where all the types correctly points to the corresponding nested classes, we add the $L$ to the top of our current program. 
Notation $R/\id$ is defined as\\*
$\begin{array}{l}
\hline
Cs_0\eq{\This{n}{C}{Cs}}=Cs_0\eq{\This{n+1}{C}{Cs}},
\text {where either } C\neq\id \text { or } n>0
\\\hline
\end{array}$

In addition of adding $1$ to all the types provided in the redirect map, since they was relative to $p$ and not $p'$, it also 
checks that $R$ actually refers to types external of $LV$, by preventing types of form
$\This{0}{\id}{\_}$.

Notation $p\op{redirectSet}{R}$
computes the set of nested classes that need to be redirected if $R$ is redirected. This is information depend just from $LV$ (the top of the program) and the domain of $R$. RedirectSet is easly computable.

\noindent $\begin{array}{l}
\hline
\dom{R} \subseteq p\op{redirectSet}{R}\\
\fop{internals}{\fop{exposedTypes}{p[\This{0}{Cs}]}} \subseteq p\op{redirectSet}{R}
\quad \text{with } Cs \in p\op{redirectSet}{R}\\
\hline
\fop{exposedTypes}{\lib{Tz}{Mz}{K?}} = Tz, \fop{exposedTypes}{Mz}\\
\fop{exposedTypes}{\Q@static@? T_0 m\rp{T_1 x_1\ldots T_n x_n} e?}= T_0\ldots T_n\\
\hline
\fop{internals}{Tz}=\{Cs\mid \This{0}{Cs} \in Tz\}
\\\hline
\end{array}$

The intuition behind \opName{redirectSet} is that if the signature of a nested class mention another nested class, they must be redirected together.
Consider the following simple example:
\begin{lstlisting}
t={A={B size()} B={} ...}
Res=t<A=String>
\end{lstlisting}
If we were to redirect \Q@A@, we would need to redirect also \Q@B@:
the type \Q@B@ is nested inside \Q@t@, thus
\Q@String@ would not be able to reach it.
The only reasonable solution is to redirect \Q@A@ and \Q@B@ together.

For our redirection (and $p'\op{bestRedirection}{}$) to be well defined, we need to check that $p\op{redirectable}{Csz}$
This is again a check local to the $LV$ (the top of the program) and is also easily computable.

\noindent $\begin{array}{l}
\hline
\fop{redirectable}{p,Csz} \text{iff}\\\quad
    \Empty \notin Csz \\\quad
    \text{if } Cs\in Csz \text{ then } \This{0}{Cs} \in \dom{p}\\\quad
    \text{if } Cs\in Csz \text{ and }C \in\dom{p(\This{0}{Cs})}
    \text{ then } Cs\Q@.@C \in Csz\\\quad
    \text{if } Cs\Q@.@C\Q@.@\_\in Csz
    \text{ then } p(\This{0}{Cs}) =\lib{\_}{C \eq{L}\_}{\_}    
\\\hline
\end{array}$

That is, the empty path is not redirectable, every nested class of a redirect path must be redirected away,
and all paths must traverse only non-private $C$.

Finally,  $p\op{bestRedirection}{R}$, given
a $p$ and an $R$ (that are valid input for redirection as defined above)
can denote the best complete map, mapping any element of $Csz$ into a suitable type in $p$.
This is the centerpiece of our formal framework and his definition will be the main topic of the next section.

Given the complete mapping $R'$, to produce the flattened result we first
remove all the elements of $Csz$ from $LV$, and then we
apply $R'$ as a rename, renaming all internal paths $Cs \in Csz$ to the corresponding external type $R'(Cs)$.
Those two notations are formally defined as following:

\noindent$\begin{array}{l}
\hline
LV\op{remove}{Cs_1\ldots Cs_n}=LV\op{remove}{Cs_1}\ldots\op{remove}{Cs_n}\\
LV[Cs\Q@.@C=\_]\op{remove}{Cs\Q@.@C}=LV \text{where } Cs\Q@.@C \notin\dom{LV}\\
\hline
R(L)=R_\Empty(L)\\
R_{Cs}(\lib{Tz}{Mz}{K?})=\lib{R_{Cs}(Tz)}{R_{Cs}(Mz)}{R_{Cs}(K?)}\\
R_{Cs}(C\eq{L})=C\eq{R_{Cs\Q@.@C}(L)}\\
R_{Cs}(M),R_{Cs}(e),R_{Cs}(K)\quad \text{ simply propagate on the structure until $T$ is reached}\\
R_{C_1\ldots C_n}(T)=\This{n+k+1}{Cs'}\quad\text{where }T\op{from}{\This0{C_1\ldots C_n}}=\This0{Cs},\ 
R(Cs)=\This{k}{Cs'}\\
\text{otherwise } R_{Cs}(T)=T
\\\hline
\end{array}$

%The second clause of \opName{remove} requires the $Cs$ to be ordered in such a way where the inner-most nested classes are removed first.
Rename must keep track of the explored $Cs$ in order to distinguish
internal paths that need to be renamed, and the mapped type need to look out of the whole explored $Cs$ and the top level code literal (thus $n+k+1$).



\begin{figure}
  \caption{Flattening}
\noindent$\begin{array}{l}
\hline
Ds\Rightarrow Ds' \text{ and } p;id\vdash E\Rightarrow E',  \text{where   
\begin{bnf}
\production{%
\ctx{V}}{\hole \mmid{}  \summ{\ctx{V}}{E} %
                \mmid{}  \summ{LV}{\ctx{V}} \mmid{} \red{\ctx{V}}{Cs}{T}}  {}
\end{bnf}}\\\hline
\\
%\inferrule[(top)]{
%	a \xrightarrow[b]{} c\quad
%	\forall i<3 a\vdash b:\text{OK}\\\\
%	\forall i<3 a\vdash b:\text{OK}
%}{
%	1+2
%	\rightarrow
%	3
%}\begin{array}{l}
%a\\b\\c
%\end{array}
%\\
\inferrule[(Top)]{
\DVz \subseteq \DVs\\\\
\DVz \vdash \textbf{Ok}\\\\
\Empty; \DVz; id \vdash E \Rightarrow E'
}{
\DVs\ \id \Q@=@ E \Ds \Rightarrow \DVs\ \id \Q@=@ E' \Ds
}\quad

\quad\quad
\inferrule[(L-enter)]{
p\op{push}{id \Q{=} L[C = E]}; C \vdash E \Rightarrow E'
}{
p; \id \vdash L[C = E] \Rightarrow L[C = E']
}

\quad\quad
\inferrule[(trait)]{
}{
p; \id \vdash t \Rightarrow p[t]
}

\\[5ex]
\inferrule[(sum)]{
LV_i=p\op{min}{\id\eq{LV_i}}\\\\
LV_1 \Q@<+@ LV_2 = LV%\\\\
%C' \textit{fresh} \\\\
%p'=p\op{push}{C'\Q@=@ LV_3}
}{
p; \id \vdash LV_1 \Q@<+@ LV_2 \Rightarrow LV
}
\quad\quad
\inferrule[(redirect)]{
  LV=p\op{min}{\id\eq{LV}}\\\\
  p' = p\op{push}{\id\eq{LV}}\\\\
  Csz = p'\op{redirectSet}{R/\id}\\\\
  p'\op{redirectable}{Csz}\\\\
  R' =p'\op{bestRedirection}{R/\id}
}{
p; \id \vdash LV \Q@<@R\Q@>@ \Rightarrow   R'(LV\op{remove}{Csz}) 
}\\[5ex]\hline
\end{array}$
\end{figure}


%We have two-top level reduction rules defining our language, of the form $Ds e ––> Ds’ e$ which simply reduces the source-code.
%The first rule $(compile)$ ‘compiles’ each top-level declaration (using a well-typed subset of allready compiled top-level declarations), this reduces the defining expresion.
%The second rule, $(main)$ is executed once all the top-level declarations have compiled (i.e. are now fully evaluated class literals), it typechecks the top-level declarations and the main expression, and then procedes to reduce it.
%In principle only one-typechecking is needed, but we repeat it to avoid declaring more rules.
%\begin{verbatim}
%DVs |- Ok
%DVs |- e : T
%DVs |- e --> e'
%(main)---------------------------------- for some type T
%DVs e --> DVs e'
%\end{verbatim}

%\begin{defye}%
%	\defy{L[C\eq{E'}]}{\lib{Tz}{\s{MV}\ C\eq{E'}\ \s{M}}{K?}}
%	\defyc{\text{where } L = \lib{Tz}{\s{MV}\ C\eq{\_}\ \s{M}}{K?}}
%	\defy{\s{T} \in p}{\forall T \in \s{T} \bullet p(T) \text{ is defined}}% WHERE WE USE IT?
%\end{defye}


%We will also use $p[T.m]$ to extra

\section{BestRedirect}
Best redirection balance three aspects:
\begin{itemize}
\item Validity: if the mapping is applied to well typed code (as in the rule \textsc{(redirect)}) then the result is still well typed.
\item Stability: changing little details on the code base (as for example adding a new unrelated nested class) do not change the selected map.
This applies to both $LV$ itself (internal stability)
and the rest of the program (external stability).
\item Specificity: when multiple options are available, the most specific is chosen.
\end{itemize}

To better divide the various aspect, we will use
functions of form $(p,R)\rightarrow Rz$, producing valid mappings
for any program $p$ and starting map $R$.
All of those functions will respect 
\opName{possibleRedirections}.
Rule \textsc{redirect} ensures 
\opName{possibleRedirections} for the input mapping,
here we check that is also verified for the complete mapping.

\noindent$\begin{array}{l}
\hline
R' \in \fop{possibleRedirections}{p, R} \text{ if }\\\quad
R \subseteq R'\\\quad
\dom{R'} = \fop{redirectSet}{p, R}\\\quad
(p, R') \in \opName{validProblems}\\
\hline
(p, Cs_1\eq{T_1}\ldots Cs_n\eq{T_n}) \in \opName{validProblems} \text{ iff }
\forall i \in 1..n:\\\quad
    p\op{minimize}{T_i}=T_i\\\quad
    T_i \text{not of form } \This0{\_}\\\quad
    p \vdash p[T] : \textbf{OK}\\\quad % // Guaranteed by our reduction rules and type-system? // TODO is this the right from?
    \fop{redirectable}{p, \fop{redirectSet}{p, R}}
%  // All of these, save for the well-typedness, are guaranteed by Redirect
\\\hline
\end{array}$

We now define $\opName{validRedirections}$ as one of such functions.
This is the most complete function achieving both validity and
internal stability. It is based on the judgement
$p \vdash T \subseteq L$
to be read as: under the program $p$,
 T is structurally a subtype of the literal $L$.
Some more auxiliary notation is used: the obvious \opName{isInterface}
and the more interesting 
\opName{superClasses} and method subtyping
$p\vdash M \leq M'$.
In \opName{superClasses} we add $T$ so that F-Bound polymorphism may work as expected, so that is possible to redirect \Q@{implements Foo}@
not only to any class implementing \Q@Foo@ but also to
\Q@Foo@ itself.
Method subtyping is given in the expressive form where the return type can be more specific, and the parameter types can be more general.

\noindent$\begin{array}{l}
\hline
R' \in \fop{validRedirections}{p,R}\ \text{iff}\\\quad
  R' \in \fop{possibleRedirections}{p, R}\\\quad
  \forall Cs \in \dom{R'}\ \, p \vdash  p[R'(Cs)] : R'(Cs) \subseteq R'(p[Cs]):Cs\\
%\end{array}$
%
%//SuperClasses is Pz',P,Any. In this way F-bound polimoprhism works as usual: {implements Foo} can be redirected to Foo
%\noindent$\begin{array}{l}
\hline
p \vdash P \subseteq \lib{Tz}{Mz}{\_}\text{ iff } \\\quad
% p|- P; {interface?' implements Pz' mwtz', ncz'} <= Cs; {interface? implements Pz mwtz, ncz}
 Tz \subseteq \fop{superClasses}{p, P}\\\quad 
 \forall m \in \dom{Mz}: \ \,
%// This implicity checks sdom(mwtz) subseteq sdom(mwtz') 
    p \vdash p[P](m) \leq Mz(m)\\\quad
 \text{if }\Q@interface@?=\Q@interface@ \text{ then  } 
%// If the LHS is not an interface
%// One can only call class methods on a non-interface, so if the RHS has them, than the LHS can't be an interface
%  // If the LHS is an interface, we need to ensure that any valid implementation of the LHS
%  // Is a valid implemention of the RHS, which requires that the RHS have the exact same method signatures as the LHS
    \forall m \in \dom{p[P]}\ \,  %// This implicity checks the sdom
      p \vdash Mz(m) \leq p[P](m)\\\quad

  \text{if } \fop{interface}{p[P]} \text{ then  }
    \Q@static@ T m\rp{Txs}\_ \notin Mz
\text{ else } \Q@interface@?=\Empty
\\
\hline
\fop{isInterface}{L} \text{iff } L=\libi\_\_\\
\hline
\fop{superClasses}{p,T}=\{T\}\cup
\fop{superClasses}{T_1}\cup\ldots\cup\fop{superClasses}{T_n}
\\\quad\text{ with }\ p[T]=\lib{T_1\ldots T_n}\_\_\\
\hline
p\vdash 
\Q@static@?\, T'_0\, m\rp{T_1 x_1\ldots T_n x_n}\_\leq
\Q@static@?\, T_0\, m\rp{T'_1 x'_1\ldots T'_n x'_n}\_
\\\quad\text{ with }
T_0\in \fop{superClasses}{p,T'_0}\ldots
T_n\in \fop{superClasses}{p,T'_n}
\\\hline
\end{array}$

Note how $\opName{validRedirections}$, while mathematically sound,
is incredibly hard to compute:
while it is easy to check if a certain 
$R' \in \fop{validRedirections}{p,R}$, finding naively all such $R'$
would require examining every possible permutation.
In particular, subtyping allows for redirections to be conceptually took out of thin-air.
Consider the following example:
\begin{lstlisting}
I=interface {..}
A= {method A m(I x)}
C={implements I ..}
t={B: {} T: {method T m(B x)}}
Res=t<T=A>
\end{lstlisting}
Clearly, selecting \Q@C@ as a candidate to complete the map is a valid choice but is also an arbitrary choice that should not be made while automatically completing the mapping. What if type \Q@D={implements I ..}@
was introduced while maintaining the program? the completed redirect map may change unpredictably.
As you can see from the former example, stability is an important requirement to allow for code maintainability.
To model stability, we define the concept of 
similar programs:

\noindent$\begin{array}{l}
\hline
DLs; DVz\, DVz'\in \fop{similarPrograms}{DLs; DVz}
\\\hline
\end{array}$

Note how we just add new declarations at the outermost level.
We will later prove that this is sufficient to ensure that 
adding/removing unrelated classes anywhere in the program would still not change the selected completed mapping.
Finally, we have all formal tools to define 
\opName{bestRedirection}, representing the high level specification of what correctly completing a mapping means.

\noindent$\begin{array}{l}
\hline
\fop{bestRedirection}{p, R} = \fop{stableMostSpecific}{p, R, \opName{validRedirections}}\\
%\hline
%\fop{stableMostSpecific}{p, R, f} = R'
% \text{ iff }\forall p' \in \fop{similarPrograms}{p}\\\quad \fop{mostSpecificRedirection}{p', f(p, R)} = R'\\
\hline
\fop{stableMostSpecific}{p, R, f} = R_0
 \text{iff } \forall p' \in \fop{similarPrograms}{p}\\\quad
      R_0 \in f(p',R)\text{ and }
\forall R_1 \in f(p',R) \ \, \fop{moreSpecific}{p,R_0,R_1}
\\
\hline
\fop{moreSpecific}{p,
  Cs_1\eq{T_1}\ldots Cs_n\eq{T_n},
  Cs_1\eq{T'_1}\ldots Cs_n\eq{T'_n}
}\\\quad
  T'_1 \in \fop{superClasses}{p,T_1}\ldots   T'_n \in \fop{superClasses}{p,T_n}
\\\hline
\end{array}$

The best redirection is a \opName{validRedirection}
that is the most specific across all similar programs.
While \opName{bestRedirection} in the current form is not
practically computable, it is clear from the formulation
a good stepping stone to obtain a computable algorithm 
would be to
replace \opName{validRedirections}
with an computable algorithm producing a subset of \opName{validRedirections} and behaving identically for
all the \opName{similarPrograms}.

If multiple solutions are available, providing one of those non deterministically would clearly break stability.
In this case, instead of just refusing to complete the mapping, we attempt to find the most specific solution: a solution where every individual mapping maps to the most specific type with respect to all the other available mappings.
Choosing the most specific solution is the desired solution in many practical cases; for example consider this variation of the former example, where
\Q@B@ is the return type instead of an argument type:

\begin{lstlisting}
I=interface { ..}
C={implements I ..}
A={C m()=..}
t={B={ } T={method B m()} ..}
Res=t<T=A>
\end{lstlisting}
\opName{bestRedirection} complete this mapping as \Q@<T=A, B=C>@ thanks to
choosing the most specific, since also \Q@B=I@ is a valid option.
In a language with a global supertype like \Q@Any@/\Q@Object@, that would
be yet another option.
Indeed, an alternative version selecting the  least specific
option may complete the mapping selecting \Q@Any@/\Q@Object@
every time a nested was declared with empty body. That in turn is very common since it is the Java equivalent of not requiring any
\Q@extends T@ constraints on a generic type.


\section{Properties of \opName{bestRedirection}}
\subsection{Internal/external stability}
\subsection{Meta-Level soundness}

\section{A computable \opName{bestRedirection}: \opName{choseRedirection}}

