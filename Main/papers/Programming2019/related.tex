
\saveSpace
\section{Related work}
\label{s:related}
\saveSpace

\textit{Type Modifiers:}
We rely on a combination of modifiers that are supported by at least 3 languages/lines of research:
L42~\cite{ServettoZucca15,ServettoEtAl13a,JOT:issue_2011_01/article1,GianniniEtAl16},
Pony~\cite{clebsch2015deny,clebsch2017orca}, and Gordon et. al.~\cite{GordonEtAl12}; 
each of these works is accompanied by proofs about the properties of those modifiers.
Since such proofs have already been done, in this work we just assume the required properties.
Those approaches all support deep/strong interpretation, without back-doors.

TM approaches like Javari~\cite{TschantzErnst05,Boyland06} and Rust~\cite{matsakis2014rust} are unsuitable since they introduce back-doors which are not easily verifiable as being used properly.
Many approaches just try to preserve purity (as for example~\cite{pearce2011jpure}), but here we also need aliasing control.
Ownership~\cite{ClarkeEtAl13,ZibinEtAl10,DietlEtAl07} is another popular form of aliasing control that can be used as a building block for static verification~\cite{%
muller2002modular,%
barnett2011specification%
}.
Capsule/isolated local variables are affine in that they can be used only once, however this linearity is a property of variables, not expressions or fields. Linear/affine types extend this idea further, however they usually do not consider the ROGs of such types, or work in an OO setting~\cite{ahmed20073,fahndrich2002adoption}.
% The interpretation of capsule/isolated local variable is linear/affine: they can be used at most 1 time. See~\cite{Smith:2000:AT:645394.651903,ahmed20073,fahndrich2002adoption} for foundational work on linear/affine types, where...

%On ownership verification
%Peter Mueller and Arnd Peotzsch Heffter,  eg Müller, P.: Modular Specification and Verification of Object-Oriented Programs, 2002.
%M. Barnett and M. Fähndrich and K. R. M. Leino and P. Müller and W. Schulte and H. Venter: Specification and Verification: The Spec# Experience. Communications of the ACM, 2011.
%\noindent\textit{Strong Exception Safety:}
%Exception safety seems at first glance a smaller issue with respect 
%to the other two, but is the final piece that lets the whole system work in a real world setting.
%Note that state of the art type systems to enforce exception safety
% do not restrict code that do not capture errors, and
%only the point of error capturing is constrained.

\textit{Object Capabilities:}
In literature, OCs are used to provide a wide range of guarantees, and many variations are present.
Object capabilities~\cite{RobustComposition}, in conjunction with type modifiers, are able to
 enforce purity of code in a modular way, without requiring the use of monads.

L42 and Gordon use OCs simply to reason about I/O and non determinism. This approach is best exemplified by Joe-E~\cite{finifter2008verifiable}, which is a self contained and minimalistic language using OCs over a subset of Java in order to reason about determinism.
However, in order for Joe-E to be a subset of Java, they leverage on a simplified model of immutability:
immutable classes must be final with only final fields that refer to immutable classes.
%Instances of immutable classes are immutable objects.
In Joe-E, every method that only takes instances of immutable classes is pure.
%\IOComm{Worth mentioning Wyvern? Since alex mentioned that it enforces purirty? Perhaps he can write that section?}
Their model however would not allow the verification of purity for invariant methods of mutable objects.
In contrast our model has a more fine grained representation of mutability: it is \emph{reference based} instead of \emph{class based}. This means that in our model, every method taking only \Q@read@ or \Q@imm@ references is pure;
both in the sense that no object visible outside of the method is mutated, but also that it is deterministic.

\textit{Class invariants protocols:}
Class invariants are a fundamental part of the design by contract methodology. 
Invariant protocols differ wildeley and can be unsound or complicated, particular due to re-entrancy and aliasing\cite{leino2004object,drossopoulou2008unified, meyer2016class}. 

While invariant protocols all seem to 
check and assume the invariant of an object after its construction, they handle invariants differently across object lifetimes; popular sound approaches include:

%\IOComm{Cite bertrand meyer? He refers to these as 'referemce leaking' and 'furtive accesses'}.

%literature on class invariant accepts that sometime the object invariant may not hold,
%and that is exacerbated because of 
%Leino, K. R. M. and Müller, P.: Object Invariants in Dynamic Contexts (ECOOP), 2004.
%S. Drossopoulou and A. Francalanza and  P. Müller and A. J. Summers: A Unified Framework for Verification Techniques for Object Invariants ECOOP 2008. 
%There are different options as to what object-invariants are known to hold:
\begin{itemize}
\item The invariants of objects in a \textit{steady} state are known to hold: that is when execution is not inside any of the objects public methods~\cite{Gopinathan:2008:RMO:1483018.1483028}. Invariants need to be constantly maintained between calls to public methods~\cite{WikiInvariant}.
\item 
%\LINE
The invariant of the receiver before a public method call and at the end of every public method body needs to be ensured. The invariant of the receiver at the beginning of a public method body and after a public method call can be assumed~\cite{Burdy2005,drossopoulou2008unified}.  
Some approaches ensure the invariant of \Q@this@ instead of the receiver of a method call~\cite{DBLP:journals/scp/MullerPL06}.
JML~\cite{JML} relaxes these requirements for helper methods, whose semantic is the same as if they where inlined.
%\LINE
%The invariant of the receiver (some approaches require the invariant of 'this' instead~\cite{?}) before a public (or non-helper~\cite{JML}) method call, and at the end of every method body needs to be ensured. The invariant of the receiver at the beginning of a public method body, and after a public method call can be assumed~\cite{Burdy2005,drossopoulou2008unified}.  


\item The same as above, but only for the bodies of `selectively exported' (i.e. non instance private) methods, and only for `qualified' (i.e. not \Q@this@) calls~\cite{meyer2016class}.
\item The invariant of an object is assumed only when a contract requires the object be `packed'. It is checked after an explicit `pack' operation, and objects can later be `unpacked'~\cite{DBLP:journals/jot/BarnettDFLS04}.
%\url{https://en.wikipedia.org/wiki/Class_invariant}}; %\item
%constantly maintained when the object is \textit{closed};
%invariant can be manually opened and closed by using special operations; % Add cite here!
\item Or, as in this work, the invariant of any object which could be \emph{involved} in execution is assumed to hold. It is checked after every modification of the object or its encapsulated ROG.
\end{itemize}

The different protocols can be deceivingly similar, and 
some approaches like JML suggest verifying a simpler approach (that method calls preserve the invariant of the \emph{receiver}) but assume a stronger one (the invariant of \emph{every} object except \Q@this@ holds).

% use the unsound option of assuming one protocol, but actually checking another.

%DONE IN INTRO breaking class invariants = bug in class code
%braking validation= DEPEND.

%To encode this range of invariant semantics
%in our approach we can add a boolean \Q@isOpen@ field and add \Q@this.isOpen || ..@
%in front of the validity condition.
%Validation can be used to manually encode complex scenarios,
%for example if a method called on an object needs to break the invariant of another object,
%it can do so by manually setting the \Q@isOpen@ flag on the other object.


%On ownership verification
%Peter Mueller and Arnd Peotzsch Heffter,  eg Müller, P.: Modular Specification and Verification of Object-Oriented Programs, 2002.
%M. Barnett and M. Fähndrich and K. R. M. Leino and P. Müller and W. Schulte and H. Venter: Specification and Verification: The Spec# Experience. Communications of the ACM, 2011.

\newcommand\sepItems{\saveSpace\saveSpace\saveSpace\\*${}_{}$\\*${}_{}\,\bullet\,$}

\textit{Runtime verification tools:}
Many languages and tools support some form of runtime invariant checking (e.g. Eiffel~\cite{Meyer:1992:EL:129093}, D~\cite{Alexandrescu:2010:DPL:1875434},
and JML~\cite{Burdy2005}).
By looking to a survey by Voigt et al.~\cite{Voigt2013} and the extensive MOP project~\cite{meredith2012overview},
it seems that most runtime verification tools (RV) empower users
to implement the kind of monitoring they see fit for their specific problem at hand. This means that users are responsible for deciding, designing, and encoding both the logical properties and the instrumentation criteria~\cite{meredith2012overview}.
In the context of class-invariants, this means the user defines the invariant protocol and the soundness of such protocol is not checked by the tool.

In practice, this means that the logic, instrumentation, and implementation end up connected:
a specific instrumentation strategy is only good to test certain logic properties in certain applications.
No guarantee is given that the implemented instrumentation strategy is able to support the required logic in the monitored application.
Some of these tools are designed to support class invariants: for example InvTS~\cite{gorbovitski08efficient} lets you write Python conditions that are verified on a set of Python objects, but the programmer needs to be able
to predict which objects are in need of being checked and to use a simpler domain specific language to target them. Hence if a programmer makes a mistake while using this domain specific language, invariant checking
will not be triggered.
Some tools are intentionally unsound and just perform invariant checking following some heuristic that is expected to catch most failures: jmlrac~\cite{Burdy2005} and Microsoft Code Contracts~\cite{fahndrich2010embedded}.

%In particular, the heuristic of 
%We encoded our GUI example also on Microsoft Code Contract; their system also ran the invariant checking $77$ times. Their system is easy to use, but it is unsound since it is built over an unsound/incomplete static verifier~\cite{?}.






%
%In this work we define a language where a minimal, standardized,
%efficient and completely general purpose instrumentation strategy can soundly verify conditions
%expressible as a\\* \Q@read method imm Bool invariant()@, for any well-typed program; with open world assumption
%and possible Byzantine behaviour of any object in the system.
%
%By seeing class invariant as a part of the type of the object,
%the `RV tool' philosophy is akin to letting the programmer customize the behaviour of the
%type system: the programmer implementation may be unsound; while our philosophy is
%to give the user a way to represent complex and expressive types (in the form of arbitrary code in 
%the \Q@invariant()@ method), but 
%the type system implementation is fixed in stone by the language designer.

Many works attempt to move out of the `RV tool' philosophy to ensure RV monitors work as expected, as for example
%\sepItems
%In avionics, where memory allocation is disallowed, making reasoning about aliasing much simpler~\cite{laurent2015assuring}:
%``\emph{Runtime Verification (RV) can act as the last line of defense to
%protect the public safety, but only if the RV system itself is trusted.}''.
%\sepItems
%In domain specific languages~\cite{ferrari2002guardians}:
%``\emph{Proof techniques for establishing security properties}''.
%\sepItems
%On assertions over restrictive domain specific languages, to tame some of the C/C++
%undefined behaviour~\cite{agten2015sound}:
%``\emph{no verified assertion in the verified
%module will ever fail at runtime, even if the module runs as part of
%a vulnerable application thSound and Unsound monitorsat is subject to code injection attacks}''.
the study of contracts as refinements of types~\cite{findler2001contract}.

Our invariant protocol is much stronger then
visible state semantics, and keeps the invariant under tight control.
Gopinathan \&al.'s.~\cite{Gopinathan:2008:RMO:1483018.1483028} approach keeps
a similar level of control:
relying on powerful aspect-oriented support, they detect any field update in the whole ROG of any object, and check all the invariants that such update may have violated.
We agree with their criticism of visible state semantics, where  methods still have to assume that any object may be broken; in such case calling any public method would trigger an error, but while the object is just passed around (and for example stored in collections), the broken state will not be detected; Gopinathan et. al. says ``\emph{there are many instances where $o$'s invariant is violated by the programmer inadvertently changing the state of $p$ when $o$ is in a steady state. Typically, $o$ and $p$ are objects exposed by the API, and the programmer (who is the user of the API), unaware of the dependency between $o$ and $p$, calls a method of $p$ in such a way that $o$'s invariant is violated. The fact that the violation occurred is detected much later, when a method of $o$ is called again, and it is difficult to determine exactly where such violations occur.}''

However, their approach addresses neither exceptions nor non-determinism caused by I/O, so their work is unsound if those aspects are taken into consideration.

Their approach is very computationally intensive, but we think it is powerful enough that it could even be used to roll back the very field update that caused the invariant to fail, making the object valid again.
We considered a roll back approach for our work, however rolling back a single field update is likely to be completely unexpected, rather we should roll back more meaningful operations, similarly to what happens
with transactional memory, and so is likely to be very hard to support efficiently.
%However we think roll-back this would be a 
%\REVComm{\REVComm{terrible}{2}{It seems in poor taste to complain of ``terrible'' ideas, especially without attempting to demonstrate the improvements of the proposed approach.}}{3}{Nontechnical term. It is not a great idea to label previous work as ``terrible''}
% ideally not only the field-update breaking the invariant should be reverted, %the roll-back should 
Using TMs to enforce strong exception safety is a much simpler alternative, providing the same level of safety, albeit being more restrictive (namely that if the operation did succeed it is still effectively rolled-back).

%: for example
%assume that we are moving object between two boats:
%the overflowing object may be removed from the \Q@cargo@ of the second boat, but it would not
%be placed back in the first boat. It would look like the object has disappeared.
%The important pTheir approach is very computationally intensive, but we think it is powerful enough that it could even be used to roll-back the very field update that caused oint here is that the program would be in an unexpected state
%even if no object invariants are violated, and this would happen \textbf{because} of the 
%invariant checking/fixing behaviour, not because of code written by the programmer.
%We believe that the only viable option is to detect violations after the fact.

%\LINE
%Another approach used in the dynamic language Racket is to interpose on primitive operations like procedure-calls and field updates; this allows one to enforce visible-state semantics by wrapping invariant operations around such operations (as is done in aspect-oriented systems like Jose~{?}). This technique can be used with gradual typing to dynamically enforce `types' of mutable structures in a safe way.

%\LINE
\emph{Chaperones and impersonators}~\cite{DBLP:conf/oopsla/StricklandTFF12} lift the techniques of gradual typing~\cite{takikawa2015towards,DBLP:conf/oopsla/TakikawaSDTF12,DBLP:conf/popl/WrigstadNLOV10}
to work on general purpose predicates, where
values can be wrapped to ensure an invariant holds.
This technique is very powerful and can be used to enforce pre and post conditions by wrapping function arguments and return values.
Their approach allows wrapping invariant operations around method calls, as is done in aspect-oriented systems like Jose~\cite{feldman2006jose}.
%One of the advantages of their system is that is transparent to the user. 
%\LINE

%\LINE
%\IOComm{ADD THOSE TO PERFORMANCE EIFFEL,D\cite{feldman2006jose,fahndrich2010embedded,abercrombie2002jcontractor,tran2003design}}

%\noindent\textit{Performance}
%Our case study shows that our sound approach can monitor programs
%for a fraction of the cost of many other approaches.
%Many other works%
%~\cite{feldman2006jose,fahndrich2010embedded,abercrombie2002jcontractor,tran2003design}
% check/run
%the invariant code at the start and end of every public
%method; this even include trivial getters.
%In  our approach, we call the \validate{} method
%one time at the end of each setter, capsule mutator method and constructor.
%We do not inject it at the end of other methods, which are usually more numerous and invoked much more often.
%Of course, \validate{} can still be called indirectly, for example by calling a setter.
%We expect our approach to result in a dramatic reduction over the number of required checks,
%except for cases when public methods just update many fields directly (without using setters).



%\LINE
\textit{Security and DMZ:}
Our approach allow verifying an object's invariant independently of the actual invariants of other objects.
This is in contrast with the main goal of static verification: to verify a method, the system assumes the contracts of other methods, and the content of those contracts is the starting point for their proof.
This is like a mathematical proof: a program is valid if it is all correct, but a single error invalidates all claims. This make it hard to perform verification on large programs, or when independently maintained third party libraries are involved.
This does not happen with a type system, since its properties are more coarse grained, simpler and easier to check.
 Static-verification has more flexible and fine-grained annotations and is more fragile since it uses a theorem prover as a back-end.

%\REVComm{
%To solve this issue, static verification systems are %starting to
%}{2}{[is this correct?] verification of reference %monitors, gradual typing, and contracts have been %explored for longer}
To soundly verify code embedded in an untrusted environment, as in gradual typing~\cite{DBLP:conf/oopsla/TakikawaSDTF12,DBLP:conf/popl/WrigstadNLOV10}, it is possible to 
consider a verified core and a run-time verified boundary.
You can see our approach as an extremely modularized	version of such system: every class is its own demilitarized zone, and the rest of the code could have Byzantine behaviour. Our formal proofs show that every class that compiles/type checks is 
soundly handled by our protocol, independently of the code that uses such class or any other surrounding code.

Our approach works both  in a library setting and the open world assumption.
Consider for example the work of Parkinson~\cite{parkinson2007class}: in his short paper he verified a property of the \Q@Subject/Observer@ pattern. However, the proof relies on (any override of) the \Q@Subject.register(Observer)@ method to respect its contract. Such assumption is unrealistic in a real world system with dynamic class loading, and could trivially be broken by a user defined \Q@EvilSubject@.

\textit{Static Verification:}
Spec\#~\cite{Barnett:2004:SPS:2131546.2131549} is a language built on top of C\#, it adds various annotations such as method contracts and class-invariants. 
It primarily follows the Boogie methodology~\cite{DBLP:journals/tcs/NaumannB06} where (implicit) annotations are used to specify and modify the owner of objects and whether their invariants are required to hold. Invariants can be \emph{ownership} based~\cite{DBLP:journals/jot/BarnettDFLS04}, where an invariant only depends on objects it owns; or \emph{visibility} based~\cite{DBLP:conf/mpc/BarnettN04,DBLP:conf/ecoop/LeinoM04}, where an invariant may depend on objects it doesn't own, provided that the class of such objects knowns about this dependence. Unlike our approach, Spec\# does not restrict the aliases that may exist for an object, rather it restricts object-mutation: an object cannot be modified if the invariant of its owner is required to hold. This is more flexible than our approach as it does not restrict aliasing and allows only part of an object's ROG to be owned/encapsulated. However as we showed in section \ref{s:patterns}, it can become much more difficult to work with and requires significant annotation since merely having an alias to an object tells you nothing about it, hampering modification and method calls. Spec\# also works with existing .NET libraries by annotating them with contracts, however such annotations are not verified. Spec\#, like use, does perform run-time checks for invariants and throws unchecked-exceptions on failure.  However Spec\# does not allow soundly recovering from an invariant failure, since catching unchecked exceptions in Spec\# is intentionally unsound.~\cite{Leino2004ExceptionSF}


Another system is AutoProof~\cite{DBLP:conf/fm/PolikarpovaTFM14}, a static-verifier for Eiffel that also follows the Boogie methodology, but extends it with \emph{semantic-collaboration} where objects keep track of their invariants dependencies using ghost-state.
Dafny~\cite{DBLP:conf/sigada/Leino12} is a new language where all code is statically verified, it supports invariants by injecting pre and post conditions following visible state semantics;
however it requires objects to be newly allocated (or cloned) before another object's invariant may depend on it.
Dafny is also generally highly restrictive with its rules for mutation, and object construction, it also does not provide any means of performing non-deterministic I/O.



%Spec\# is statically verified wheras we rely on a type system: we have 4 type-modifiers that can be applied anywhere a type can be used (like a variable declaration) and the type-system uses a small set of fixed deterministic rules.
%Wheras the static-verification aproach has much more flexible and fine-grained annotations (with meth pre/post conditions) and uses a theoreom prover as a back-end, this can make it harder for users to program as it is not obvious whether the theorom prover will accept a program or not.
% In addition, the approach of a static-verifier can also be non-modular: changes to the body of one method could affect whether another is verified.

%many works on static verification, such as thoser Spec\#~\cite{??}


\textit{Specification languages:}
Using a specification language based on the mathematical metalanguage and different from the program language's semantics may seem attractive, since it can express uncomputable concepts and has no mutation or non determinism, and is often easier to formally reason about.

However a study~\cite{chalin2007logical} discovered that developers expect specification languages to follow the semantics of the underling language, including short-circuit semantics and arithmetic exceptions; thus for example \Q@1/0 || 2>1@ should not hold, while \Q@2>1 || 1/0@ should hold, thanks to short-circuiting.
This study was influential enough to convince JML to change its interpretation of logical expressions
accordingly~\cite{chalin2008jml}.
Dafny~\cite{DBLP:conf/sigada/Leino12} uses a hybrid approach: it has mostly the same language for both specification and execution. Specification (`ghost') contexts can use uncomputable constructs such as universal quantification over infinite sets. Whereas runtime contexts allow mutation, object allocation and print statements. The semantics of shared constructs (such as short circuiting logic operators) is the same in both contexts.

Most runtime verification systems, such as ours, use a meta-circular approach: specifications are simply code in the underlying language. Since specifications are checked at runtime, they are unable to verify uncomputable contracts.
 Ensuring determinism in a non-functional language is challenging. Spec\# recognizes the need for purity/determinism when method calls are allowed in contracts~\cite{barnett200499} `\emph{There are three main current approaches: a) forbid the use of functions in specifications, b) allow only provably pure functions, or c) allow programmers free use
	of functions. The first approach is not scalable, the second overly restrictive and
	the third unsound.}'.

They recognize that many tools unsoundly use option (c), such as AsmL~\cite{barnett2003runtime}.
Spec\# aims to follow (b) but only considers non-determinism caused by memory mutation, and allows other non-deterministic operations, such as I/O and random number generation. For example, the following method verifies:
\begin{lstlisting}
[Pure] bool uncertain() {return new Random().Next() %$$ 2 == 0;}
\end{lstlisting}
And so \Q@assert uncertain() == uncertain();@ also verifies, but randomly fails with an exception at runtime.
As you can see failing to handle non-determinism jeopardises reasoning.

A simpler and more restrictive solution to these problems is to prevent `pure' functions from reading or writing to any non-final fields, or calling any impure functions. This is the approach used by~\cite{Flanagan06hybridtypes}, one advantage of their approach is that invariants (which must be `pure') can read from a chain of final fields, even when they are contained in otherwise mutable objects. However their approach completely prevents invariants from mutating newly allocated objects, thus greatly restricting how computations can be performed.

%allowing mutation can cause specifications to affect the operational behaviour of code, which is against their purpose.

% They propose a concept of observational purity, that if completely fleshed out could possibly be a great addition to our proposed system. We speculate that some  primitive language support may be needed, for example implementing the Flyweight pattern as part of the language semantics.


% The most friendly aattractive to specify contracts (such as class invariants) in the same source language as the code. This common approach is used by most runtime verification systems, like our work, Eiffel and D. However their



% What's wrong with using the same language for both specification and implementation?
% What are peoples approaches



%${}_{}$\sepItems









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%1 aliasing control
%  example hamster can be broken with those 2 lines
%2 I/O /determinism
%  hamster EvilPoint with random equal is accepted
%3 exceptions
%  spec sharp is happy to be unsound with capturing %unchecked exceptions
%---
%*TMs, OCs
%
%*expand on invariant protocol
%
%*RV tool
%------------
%*spec# unsond, parkinson critique, and static %verification is like math proof
%
%*Soundness or not
%
%*C#purity, dedicate spec language



%\noindent\textit{Theorem provers and SAT solvers}
%Rather than providing a simple set of rules as to what a \Q@validate@ method can contain,
%and where to insert calls to it, we could instead rely on implementation-specific static analysis:
% in which a \Q@validate@ method is valid iff the compiler can prove that it is deterministic
% and that it’s generated \Q@validate()@ calls are sufficient to enforce validation.
%Though approaches like this are frequently used such as with unifying Java’s generic-wildcards [], Rust’s ‘borrow checker’, …; we believe that would not produce a good result for our purposes: 
%\begin{itemize}
%	\item it would mean that a programmer would have no way of telling whether their code would compile, in particular code compiling would depend on the specific compiler (version) used.
%	\item the runtime cost of validation would be completely unpridictibable; since it is deterministic there is nothing stopping the compiler from calling \Q@validate@ any number of times, and at any point in time.
%	\item When a validation error could be throw would likewise be unpredictable, though it should happen after an object is made invalid\footnote{technically our definition of validation technically allows the error to happen sooner, as long as it’s not too late; however pre-emptive errors like this would be extremely hard to debug}, it could happen any time before it’s use. Making matters worse, if multiple object’s would be invalidated before either is used, which one’s error would be thrown is unconstrained
%	\item This approach will not work well in the pressence of dynamic code loading, in particular it woud likley significantly slow down such loading or spurioslly fail depending on what other code has been loaded
%\end{itemize}



%Conclusions? future work?
%@StrongExceptionSafety is 
%a very strong property,
%and some languages may be unwilling to commit to always preserve it.
%In particular, depending on the details of a specific language
% releasing resources as in \Q@finally@ blocks may require
%some relaxation of @StrongExceptionSafety. Sound releasing of resources could be interesting
%future work.
